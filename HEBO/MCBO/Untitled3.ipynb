{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "a6e84878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 12)\n",
      "(714, 12)\n",
      "(1, 25) xnextdf\n",
      "      W1_00     W1_01     W1_02     W1_03     W1_10     W1_11     W1_12  \\\n",
      "0  0.694593 -0.075086 -0.016972 -0.506505  0.531527 -0.352209 -0.059352   \n",
      "\n",
      "      W1_13     W1_20     W1_21  ...     W1_33      b1_0      b1_1      b1_2  \\\n",
      "0  0.617955  0.170972  0.964155  ...  0.439117 -0.723536  0.104445  0.441786   \n",
      "\n",
      "       b1_3     W2_00     W2_01     W2_02     W2_03      b2_0  \n",
      "0 -0.265652  0.464602 -0.577474  0.753196 -0.407304  0.247507  \n",
      "\n",
      "[1 rows x 25 columns] --------------------------------------------\n",
      "Evaluating with parameters:\n",
      "x_ind: [ 0.69459349 -0.07508637 -0.01697155 -0.50650516  0.53152664 -0.35220889\n",
      " -0.0593519   0.6179555   0.17097188  0.96415476 -0.05648738 -0.91623921\n",
      "  0.37940307  0.28710425  0.39301437  0.43911683 -0.72353583  0.10444456\n",
      "  0.44178644 -0.26565237  0.46460154 -0.57747417  0.75319624 -0.40730385\n",
      "  0.24750677]\n",
      "xind shape (25,)\n",
      "(1, 25) xnextdf\n",
      "      W1_00     W1_01     W1_02     W1_03     W1_10     W1_11     W1_12  \\\n",
      "0 -0.117451  0.881932  0.951055 -0.099809 -0.817034 -0.727499  0.747151   \n",
      "\n",
      "      W1_13     W1_20     W1_21  ...     W1_33      b1_0      b1_1      b1_2  \\\n",
      "0  0.600728 -0.368693 -0.217816  ... -0.785094 -0.201919  0.321948  0.456942   \n",
      "\n",
      "       b1_3     W2_00     W2_01    W2_02     W2_03      b2_0  \n",
      "0 -0.962238  0.221893  0.265354  0.82497  0.451136 -0.988737  \n",
      "\n",
      "[1 rows x 25 columns] --------------------------------------------\n",
      "Evaluating with parameters:\n",
      "x_ind: [-0.11745133  0.88193158  0.95105466 -0.09980946 -0.81703408 -0.72749887\n",
      "  0.74715061  0.60072838 -0.36869275 -0.21781551 -0.00836105  0.00613665\n",
      "  0.44974171 -0.82398794 -0.24791642 -0.78509357 -0.2019191   0.32194764\n",
      "  0.45694189 -0.96223824  0.22189305  0.26535407  0.82497011  0.4511365\n",
      " -0.98873737]\n",
      "xind shape (25,)\n",
      "(1, 25) xnextdf\n",
      "      W1_00     W1_01     W1_02     W1_03     W1_10     W1_11     W1_12  \\\n",
      "0 -0.964084 -0.027256 -0.333517 -0.955731  0.027752  0.775126  0.525598   \n",
      "\n",
      "      W1_13     W1_20     W1_21  ...     W1_33      b1_0      b1_1      b1_2  \\\n",
      "0 -0.085952 -0.826121 -0.840035  ...  0.692238  0.959829 -0.354198 -0.999894   \n",
      "\n",
      "       b1_3     W2_00     W2_01     W2_02     W2_03      b2_0  \n",
      "0 -0.798939  0.640586  0.952028 -0.468052 -0.540348 -0.627366  \n",
      "\n",
      "[1 rows x 25 columns] --------------------------------------------\n",
      "Evaluating with parameters:\n",
      "x_ind: [-0.96408427 -0.0272558  -0.33351652 -0.9557311   0.02775167  0.77512614\n",
      "  0.52559811 -0.0859521  -0.82612088 -0.84003503  0.46447583  0.16844958\n",
      "  0.39623385  0.78706917  0.88521942  0.69223837  0.95982887 -0.35419786\n",
      " -0.99989435 -0.79893928  0.64058592  0.95202785 -0.46805187 -0.54034792\n",
      " -0.62736645]\n",
      "xind shape (25,)\n",
      "(1, 25) xnextdf\n",
      "      W1_00     W1_01     W1_02    W1_03     W1_10     W1_11     W1_12  \\\n",
      "0 -0.850979 -0.074761  0.190093  0.28142 -0.536834 -0.058475  0.026154   \n",
      "\n",
      "      W1_13   W1_20     W1_21  ...     W1_33      b1_0      b1_1      b1_2  \\\n",
      "0  0.213024 -0.5541 -0.820348  ...  0.471436 -0.548237 -0.037452  0.907842   \n",
      "\n",
      "       b1_3    W2_00     W2_01     W2_02     W2_03      b2_0  \n",
      "0 -0.007561 -0.19178 -0.378798 -0.075719 -0.998188 -0.800548  \n",
      "\n",
      "[1 rows x 25 columns] --------------------------------------------\n",
      "Evaluating with parameters:\n",
      "x_ind: [-0.85097908 -0.07476146  0.19009269  0.28142045 -0.53683402 -0.05847522\n",
      "  0.02615444  0.21302445 -0.55409998 -0.82034849 -0.82445417  0.84039397\n",
      " -0.37805396 -0.22146721  0.63596954  0.47143597 -0.54823717 -0.0374521\n",
      "  0.90784161 -0.00756104 -0.1917803  -0.37879806 -0.07571905 -0.99818833\n",
      " -0.80054759]\n",
      "xind shape (25,)\n",
      "(1, 25) xnextdf\n",
      "      W1_00     W1_01     W1_02     W1_03     W1_10    W1_11     W1_12  \\\n",
      "0  0.020785  0.570144 -0.819609 -0.172148 -0.637564  0.75623  0.341494   \n",
      "\n",
      "      W1_13     W1_20     W1_21  ...     W1_33      b1_0      b1_1      b1_2  \\\n",
      "0  0.208519 -0.623883  0.181507  ... -0.576566  0.224317  0.234709  0.628418   \n",
      "\n",
      "       b1_3     W2_00     W2_01     W2_02     W2_03      b2_0  \n",
      "0  0.379929 -0.556998 -0.078061  0.098288  0.430474 -0.273351  \n",
      "\n",
      "[1 rows x 25 columns] --------------------------------------------\n",
      "Evaluating with parameters:\n",
      "x_ind: [ 0.02078544  0.57014372 -0.8196086  -0.17214788 -0.63756391  0.75623039\n",
      "  0.3414944   0.20851903 -0.62388287  0.18150744  0.33636132 -0.90168295\n",
      " -0.80067216 -0.37326119  0.17538894 -0.57656644  0.22431732  0.2347085\n",
      "  0.62841753  0.37992871 -0.5569979  -0.07806082  0.09828803  0.43047371\n",
      " -0.27335105]\n",
      "xind shape (25,)\n",
      "(1, 25) xnextdf\n",
      "      W1_00     W1_01    W1_02     W1_03     W1_10     W1_11    W1_12  \\\n",
      "0  0.938879  0.643155  0.21203 -0.459883  0.620607  0.171704  0.69471   \n",
      "\n",
      "      W1_13     W1_20     W1_21  ...     W1_33      b1_0      b1_1      b1_2  \\\n",
      "0  0.654316 -0.143961  0.198727  ... -0.709428 -0.134054 -0.190178  0.807963   \n",
      "\n",
      "       b1_3     W2_00     W2_01     W2_02     W2_03      b2_0  \n",
      "0 -0.330529  0.024047  0.348003  0.898225 -0.657631  0.002702  \n",
      "\n",
      "[1 rows x 25 columns] --------------------------------------------\n",
      "Evaluating with parameters:\n",
      "x_ind: [ 0.93887875  0.64315538  0.21203    -0.45988307  0.6206066   0.17170424\n",
      "  0.69471011  0.6543163  -0.1439613   0.1987271   0.7916242   0.14699917\n",
      " -0.45506563 -0.07527711 -0.49563604 -0.70942798 -0.1340539  -0.19017837\n",
      "  0.80796339 -0.33052935  0.02404678  0.34800274  0.89822491 -0.65763051\n",
      "  0.00270199]\n",
      "xind shape (25,)\n",
      "(1, 25) xnextdf\n",
      "      W1_00     W1_01     W1_02     W1_03     W1_10     W1_11     W1_12  \\\n",
      "0  0.386419  0.108978 -0.820736 -0.586326 -0.729924  0.348384 -0.980922   \n",
      "\n",
      "      W1_13     W1_20     W1_21  ...     W1_33      b1_0      b1_1      b1_2  \\\n",
      "0 -0.149995  0.316568 -0.162482  ... -0.367415 -0.857951 -0.603255 -0.968713   \n",
      "\n",
      "       b1_3     W2_00     W2_01     W2_02     W2_03      b2_0  \n",
      "0  0.390977  0.097684  0.083013  0.997235 -0.791035 -0.913206  \n",
      "\n",
      "[1 rows x 25 columns] --------------------------------------------\n",
      "Evaluating with parameters:\n",
      "x_ind: [ 0.38641908  0.10897823 -0.82073645 -0.58632578 -0.72992373  0.34838423\n",
      " -0.98092209 -0.14999517  0.31656757 -0.16248202 -0.88442361  0.70725986\n",
      "  0.6635706  -0.21986787 -0.59256521 -0.3674153  -0.85795086 -0.60325492\n",
      " -0.96871341  0.39097692  0.09768351  0.08301256  0.99723464 -0.79103521\n",
      " -0.91320564]\n",
      "xind shape (25,)\n",
      "(1, 25) xnextdf\n",
      "      W1_00     W1_01     W1_02     W1_03     W1_10     W1_11    W1_12  \\\n",
      "0 -0.858168 -0.640404  0.918471  0.308046 -0.062697  0.608522 -0.19422   \n",
      "\n",
      "      W1_13     W1_20     W1_21  ...     W1_33      b1_0      b1_1      b1_2  \\\n",
      "0 -0.449107 -0.472388 -0.136313  ...  0.952141 -0.024247  0.923281  0.867718   \n",
      "\n",
      "       b1_3     W2_00     W2_01     W2_02     W2_03      b2_0  \n",
      "0  0.397271  0.076653  0.686856 -0.529186 -0.524968 -0.303044  \n",
      "\n",
      "[1 rows x 25 columns] --------------------------------------------\n",
      "Evaluating with parameters:\n",
      "x_ind: [-0.85816762 -0.64040437  0.91847087  0.30804616 -0.06269673  0.60852231\n",
      " -0.19421994 -0.44910664 -0.47238785 -0.13631262 -0.71869332 -0.51614454\n",
      "  0.9609416   0.72715401 -0.31923032  0.95214117 -0.02424679  0.92328098\n",
      "  0.86771759  0.39727118  0.07665274  0.68685559 -0.5291855  -0.52496776\n",
      " -0.30304405]\n",
      "xind shape (25,)\n",
      "(1, 25) xnextdf\n",
      "      W1_00     W1_01     W1_02     W1_03     W1_10     W1_11     W1_12  \\\n",
      "0 -0.621278 -0.064484  0.431325 -0.167566  0.877156  0.318311  0.814486   \n",
      "\n",
      "     W1_13    W1_20     W1_21  ...     W1_33      b1_0      b1_1      b1_2  \\\n",
      "0 -0.46993 -0.97586 -0.892135  ... -0.486337  0.404329  0.022935  0.619577   \n",
      "\n",
      "       b1_3     W2_00     W2_01    W2_02     W2_03      b2_0  \n",
      "0  0.407872  0.130148  0.523485  0.45396 -0.240199 -0.641641  \n",
      "\n",
      "[1 rows x 25 columns] --------------------------------------------\n",
      "Evaluating with parameters:\n",
      "x_ind: [-0.62127825 -0.06448354  0.4313253  -0.16756594  0.87715648  0.31831109\n",
      "  0.81448633 -0.46993016 -0.97586013 -0.89213483  0.43326387  0.7525847\n",
      " -0.01230597  0.85852195 -0.04686411 -0.48633685  0.40432866  0.02293508\n",
      "  0.61957706  0.40787181  0.13014758  0.52348541  0.45395974 -0.24019896\n",
      " -0.64164065]\n",
      "xind shape (25,)\n",
      "(1, 25) xnextdf\n",
      "      W1_00     W1_01    W1_02     W1_03     W1_10    W1_11     W1_12  \\\n",
      "0  0.542646  0.936145  0.57632 -0.640681 -0.628336 -0.20389  0.480626   \n",
      "\n",
      "      W1_13     W1_20     W1_21  ...     W1_33     b1_0      b1_1      b1_2  \\\n",
      "0  0.072385  0.037875  0.033093  ... -0.087746  0.02644  0.467841  0.674438   \n",
      "\n",
      "       b1_3    W2_00     W2_01     W2_02     W2_03     b2_0  \n",
      "0 -0.014956  0.89741 -0.883406 -0.243938 -0.048187  0.29414  \n",
      "\n",
      "[1 rows x 25 columns] --------------------------------------------\n",
      "Evaluating with parameters:\n",
      "x_ind: [ 0.54264629  0.93614501  0.57631958 -0.64068142 -0.62833589 -0.20389032\n",
      "  0.48062606  0.07238456  0.03787473  0.03309302 -0.1254367  -0.96191318\n",
      " -0.98059929 -0.39309357 -0.40786898 -0.08774558  0.02643995  0.46784074\n",
      "  0.67443829 -0.01495629  0.89740984 -0.88340559 -0.24393788 -0.04818735\n",
      "  0.29414033]\n",
      "xind shape (25,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 25) xnextdf\n",
      "     W1_00     W1_01     W1_02     W1_03     W1_10     W1_11     W1_12  \\\n",
      "0  0.68288  0.505972  0.408209  0.378218  0.955987  0.027267  0.085683   \n",
      "\n",
      "      W1_13     W1_20     W1_21  ...    W1_33      b1_0      b1_1      b1_2  \\\n",
      "0 -0.101704  0.211519  0.009479  ... -0.93996  0.744212 -0.177906 -0.813425   \n",
      "\n",
      "       b1_3     W2_00     W2_01     W2_02    W2_03      b2_0  \n",
      "0  0.829772  0.804491  0.996842 -0.983784 -0.91086 -0.071455  \n",
      "\n",
      "[1 rows x 25 columns] --------------------------------------------\n",
      "Evaluating with parameters:\n",
      "x_ind: [ 0.68287952  0.50597239  0.40820872  0.37821816  0.95598685  0.02726677\n",
      "  0.08568275 -0.10170359  0.21151898  0.00947895 -0.19290276  0.46490926\n",
      " -0.71991038  0.72787161  0.02985899 -0.93995955  0.74421213 -0.17790561\n",
      " -0.81342523  0.8297723   0.80449125  0.99684247 -0.98378361 -0.91085992\n",
      " -0.07145526]\n",
      "xind shape (25,)\n",
      "(1, 25) xnextdf\n",
      "      W1_00     W1_01     W1_02     W1_03     W1_10     W1_11     W1_12  \\\n",
      "0  0.235389  0.622325  0.179458  0.130145  0.988576  0.606091  0.059289   \n",
      "\n",
      "      W1_13     W1_20     W1_21  ...     W1_33      b1_0      b1_1      b1_2  \\\n",
      "0 -0.119758  0.925741 -0.700901  ... -0.205545 -0.422126 -0.289736  0.477556   \n",
      "\n",
      "       b1_3     W2_00     W2_01     W2_02     W2_03      b2_0  \n",
      "0 -0.021607  0.751434  0.504805 -0.981495  0.941147 -0.009365  \n",
      "\n",
      "[1 rows x 25 columns] --------------------------------------------\n",
      "Evaluating with parameters:\n",
      "x_ind: [ 0.23538917  0.62232521  0.17945807  0.13014457  0.98857597  0.60609105\n",
      "  0.05928905 -0.11975846  0.92574099 -0.70090136 -0.72606673  0.86622644\n",
      " -0.35583885  0.56304626 -0.66925915 -0.20554494 -0.42212597 -0.28973647\n",
      "  0.47755606 -0.02160696  0.75143407  0.50480491 -0.98149496  0.94114667\n",
      " -0.00936471]\n",
      "xind shape (25,)\n",
      "(1, 25) xnextdf\n",
      "      W1_00     W1_01     W1_02     W1_03     W1_10     W1_11    W1_12  \\\n",
      "0  0.288846 -0.934349 -0.100174  0.717344  0.111234 -0.252699 -0.30446   \n",
      "\n",
      "      W1_13     W1_20     W1_21  ...    W1_33      b1_0      b1_1      b1_2  \\\n",
      "0 -0.817725  0.178942 -0.792321  ...  0.14264 -0.827502 -0.185671  0.919833   \n",
      "\n",
      "      b1_3     W2_00     W2_01     W2_02     W2_03      b2_0  \n",
      "0 -0.09947  0.602834  0.727807 -0.078868  0.432126 -0.144616  \n",
      "\n",
      "[1 rows x 25 columns] --------------------------------------------\n",
      "Evaluating with parameters:\n",
      "x_ind: [ 0.28884634 -0.934349   -0.10017444  0.71734411  0.11123431 -0.25269941\n",
      " -0.3044598  -0.81772532  0.17894161 -0.79232123  0.01354008  0.31439532\n",
      "  0.1571707   0.86161534  0.95542563  0.14264003 -0.82750166 -0.1856714\n",
      "  0.91983338 -0.09946961  0.60283362  0.72780663 -0.07886798  0.43212611\n",
      " -0.14461642]\n",
      "xind shape (25,)\n",
      "(1, 25) xnextdf\n",
      "      W1_00     W1_01     W1_02     W1_03     W1_10     W1_11     W1_12  \\\n",
      "0 -0.943325 -0.064281  0.716747 -0.765048 -0.659715 -0.223757  0.041041   \n",
      "\n",
      "      W1_13     W1_20     W1_21  ...     W1_33      b1_0      b1_1      b1_2  \\\n",
      "0  0.437207 -0.057655 -0.752855  ...  0.906249  0.873131 -0.181607  0.357095   \n",
      "\n",
      "       b1_3     W2_00     W2_01    W2_02     W2_03      b2_0  \n",
      "0 -0.601238  0.886856  0.302395 -0.02333 -0.353612  0.912039  \n",
      "\n",
      "[1 rows x 25 columns] --------------------------------------------\n",
      "Evaluating with parameters:\n",
      "x_ind: [-0.9433251  -0.06428107  0.71674676 -0.7650482  -0.65971452 -0.22375709\n",
      "  0.04104107  0.43720748 -0.05765488 -0.75285502 -0.74690938 -0.12194151\n",
      " -0.84981271 -0.50849938 -0.78415089  0.90624923  0.87313122 -0.18160671\n",
      "  0.35709468 -0.60123836  0.88685621  0.30239542 -0.02333017 -0.35361187\n",
      "  0.91203942]\n",
      "xind shape (25,)\n",
      "(1, 25) xnextdf\n",
      "      W1_00     W1_01     W1_02     W1_03    W1_10    W1_11     W1_12  \\\n",
      "0  0.657061  0.293242  0.913671  0.571333 -0.36779 -0.80862  0.310766   \n",
      "\n",
      "      W1_13     W1_20     W1_21  ...     W1_33      b1_0     b1_1      b1_2  \\\n",
      "0 -0.572527  0.146803 -0.797944  ...  0.033472 -0.243607  0.08836  0.141123   \n",
      "\n",
      "      b1_3     W2_00    W2_01     W2_02     W2_03     b2_0  \n",
      "0 -0.38469 -0.803011  0.71257 -0.954861 -0.362605 -0.57053  \n",
      "\n",
      "[1 rows x 25 columns] --------------------------------------------\n",
      "Evaluating with parameters:\n",
      "x_ind: [ 0.65706052  0.29324227  0.91367134  0.5713332  -0.36779022 -0.80861977\n",
      "  0.3107664  -0.57252662  0.14680325 -0.79794351 -0.04675909 -0.91023797\n",
      "  0.8867146   0.1539183   0.86388828  0.03347209 -0.2436073   0.08836023\n",
      "  0.14112281 -0.38468983 -0.80301072  0.71257026 -0.95486061 -0.36260549\n",
      " -0.57052975]\n",
      "xind shape (25,)\n",
      "(1, 25) xnextdf\n",
      "     W1_00     W1_01     W1_02     W1_03     W1_10     W1_11     W1_12  \\\n",
      "0  0.57655  0.660065 -0.001928  0.347034 -0.151014  0.829169 -0.163274   \n",
      "\n",
      "      W1_13     W1_20     W1_21  ...     W1_33      b1_0     b1_1      b1_2  \\\n",
      "0 -0.606093  0.855801  0.219416  ...  0.638215  0.152811  0.08694  0.891259   \n",
      "\n",
      "       b1_3     W2_00     W2_01     W2_02     W2_03      b2_0  \n",
      "0 -0.915331 -0.469753 -0.775234  0.373855 -0.102915  0.033329  \n",
      "\n",
      "[1 rows x 25 columns] --------------------------------------------\n",
      "Evaluating with parameters:\n",
      "x_ind: [ 0.57655042  0.66006537 -0.00192765  0.34703357 -0.15101433  0.82916887\n",
      " -0.16327392 -0.60609274  0.85580126  0.21941639  0.5272306   0.84022763\n",
      " -0.26983265 -0.45091083  0.82715602  0.63821465  0.1528111   0.08693974\n",
      "  0.89125936 -0.91533101 -0.46975348 -0.77523405  0.37385483 -0.1029149\n",
      "  0.03332852]\n",
      "xind shape (25,)\n",
      "(1, 25) xnextdf\n",
      "     W1_00     W1_01    W1_02     W1_03     W1_10     W1_11     W1_12  \\\n",
      "0 -0.48193 -0.901453  0.86307 -0.769978 -0.381352 -0.817927  0.428306   \n",
      "\n",
      "      W1_13     W1_20     W1_21  ...     W1_33      b1_0      b1_1      b1_2  \\\n",
      "0 -0.174483  0.656756 -0.607035  ...  0.094255  0.584006 -0.005692 -0.114126   \n",
      "\n",
      "       b1_3     W2_00     W2_01     W2_02    W2_03      b2_0  \n",
      "0  0.267071 -0.009888 -0.304524 -0.411993  0.25249  0.472035  \n",
      "\n",
      "[1 rows x 25 columns] --------------------------------------------\n",
      "Evaluating with parameters:\n",
      "x_ind: [-0.48192971 -0.90145269  0.8630701  -0.76997813 -0.38135155 -0.81792737\n",
      "  0.42830629 -0.17448336  0.65675629 -0.6070347  -0.48347128 -0.05940755\n",
      "  0.82516844  0.64818898  0.05072096  0.09425529  0.58400625 -0.00569237\n",
      " -0.11412576  0.26707059 -0.00988811 -0.30452433 -0.41199322  0.25248965\n",
      "  0.47203474]\n",
      "xind shape (25,)\n",
      "(1, 25) xnextdf\n",
      "      W1_00     W1_01     W1_02     W1_03     W1_10     W1_11     W1_12  \\\n",
      "0  0.417353  0.704878  0.166666  0.389641 -0.438729  0.296418  0.722464   \n",
      "\n",
      "      W1_13     W1_20    W1_21  ...    W1_33      b1_0      b1_1      b1_2  \\\n",
      "0 -0.544443 -0.354043 -0.49714  ... -0.91843 -0.351373 -0.730428  0.198587   \n",
      "\n",
      "       b1_3     W2_00     W2_01     W2_02     W2_03      b2_0  \n",
      "0 -0.502929  0.693069  0.752842  0.119786 -0.707254  0.113689  \n",
      "\n",
      "[1 rows x 25 columns] --------------------------------------------\n",
      "Evaluating with parameters:\n",
      "x_ind: [ 0.41735344  0.70487777  0.16666582  0.38964146 -0.43872884  0.29641835\n",
      "  0.72246445 -0.54444332 -0.35404253 -0.4971397  -0.03842766 -0.94200651\n",
      "  0.56104361 -0.98935162  0.44982054 -0.91842997 -0.35137267 -0.73042843\n",
      "  0.19858713 -0.50292851  0.69306899  0.75284189  0.1197862  -0.70725387\n",
      "  0.11368883]\n",
      "xind shape (25,)\n",
      "(1, 25) xnextdf\n",
      "      W1_00     W1_01     W1_02     W1_03     W1_10     W1_11    W1_12  \\\n",
      "0 -0.570291  0.096802  0.076768 -0.116905  0.944459 -0.173611 -0.69063   \n",
      "\n",
      "      W1_13     W1_20     W1_21  ...     W1_33      b1_0      b1_1      b1_2  \\\n",
      "0  0.167717 -0.525491 -0.652856  ... -0.369983  0.432329 -0.007849 -0.047323   \n",
      "\n",
      "       b1_3     W2_00    W2_01     W2_02    W2_03      b2_0  \n",
      "0 -0.391689  0.010331 -0.59669 -0.398162 -0.62214  0.858042  \n",
      "\n",
      "[1 rows x 25 columns] --------------------------------------------\n",
      "Evaluating with parameters:\n",
      "x_ind: [-0.57029079  0.09680187  0.07676837 -0.11690542  0.94445942 -0.17361104\n",
      " -0.6906303   0.16771653 -0.52549127 -0.65285571  0.98403194  0.37964214\n",
      "  0.51087455  0.12963833 -0.1279034  -0.36998345  0.43232908 -0.00784904\n",
      " -0.04732341 -0.39168949  0.01033142 -0.59668958 -0.39816185 -0.62214037\n",
      "  0.85804177]\n",
      "xind shape (25,)\n",
      "(1, 25) xnextdf\n",
      "     W1_00     W1_01     W1_02    W1_03    W1_10     W1_11     W1_12  \\\n",
      "0 -0.91934 -0.605372 -0.091035  0.88997 -0.69327 -0.188761 -0.096906   \n",
      "\n",
      "      W1_13     W1_20     W1_21  ...     W1_33      b1_0      b1_1      b1_2  \\\n",
      "0 -0.257073 -0.474733  0.915671  ...  0.151099 -0.729162 -0.661792  0.865692   \n",
      "\n",
      "       b1_3     W2_00     W2_01     W2_02     W2_03      b2_0  \n",
      "0  0.541629  0.496069 -0.843516 -0.665731  0.718869 -0.244063  \n",
      "\n",
      "[1 rows x 25 columns] --------------------------------------------\n",
      "Evaluating with parameters:\n",
      "x_ind: [-0.91933968 -0.60537206 -0.09103453  0.88996957 -0.69327035 -0.18876122\n",
      " -0.09690595 -0.25707332 -0.47473278  0.91567061  0.37330085 -0.03632549\n",
      "  0.21343916 -0.32159314  0.26006916  0.15109902 -0.7291623  -0.6617922\n",
      "  0.86569244  0.54162904  0.49606948 -0.8435158  -0.66573137  0.71886917\n",
      " -0.24406329]\n",
      "xind shape (25,)\n",
      "Best parameters:       W1_00     W1_01     W1_02     W1_03    W1_10    W1_11     W1_12  \\\n",
      "0  0.657061  0.293242  0.913671  0.571333 -0.36779 -0.80862  0.310766   \n",
      "\n",
      "      W1_13     W1_20     W1_21  ...     W1_33      b1_0     b1_1      b1_2  \\\n",
      "0 -0.572527  0.146803 -0.797944  ...  0.033472 -0.243607  0.08836  0.141123   \n",
      "\n",
      "      b1_3     W2_00    W2_01     W2_02     W2_03     b2_0  \n",
      "0 -0.38469 -0.803011  0.71257 -0.954861 -0.362605 -0.57053  \n",
      "\n",
      "[1 rows x 25 columns]\n",
      "Best loss: 0.4466160086596127\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import List, Tuple, Any, Dict\n",
    "from mcbo.optimizers.bo_builder import BoBuilder\n",
    "from mcbo.tasks.task_base import TaskBase\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "print(df.shape)\n",
    "df = df.dropna(subset=['Age', 'Fare', 'Sex'])\n",
    "target_var = df.Survived\n",
    "print(df.shape)\n",
    "\n",
    "df = df[['Sex','Age', 'Fare']]\n",
    "encode_cat_var = pd.get_dummies(df['Sex'])\n",
    "df = df.drop(['Sex'], axis =1)\n",
    "X = pd.concat([encode_cat_var, df], axis = 1)\n",
    "X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns) \n",
    "\n",
    "y = target_var\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Activation functions\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def softmax(Z):\n",
    "    return np.exp(Z) / np.sum(np.exp(Z))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Loss function\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    '''epsilon = 1e-15  # Small value to avoid numerical instability\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip predicted probabilities to avoid taking the log of zero or one\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "'''\n",
    "    cost = np.mean(np.abs(y_true - y_pred))  # Example: Mean absolute error\n",
    "    return cost\n",
    "\n",
    "# Feed-forward ANN\n",
    "def forward_prop(W1, b1, W2, b2, X_data):\n",
    "    n_features = X_data.shape[1]\n",
    "    if W1.shape[1] != n_features:\n",
    "        W1_shape_0 = W1.size / n_features \n",
    "        W1 = np.reshape(W1_shape_0 ,n_features)\n",
    "    if W2.shape[1] != W1.shape[0]:\n",
    "        W2_shape_0 = W2.size / W1.shape[0]\n",
    "        W2 = np.reshape((W2_shape_0, W1.shape[0]))\n",
    "    Z1 = np.dot(W1, X_data.T) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2).T \n",
    "    return A2\n",
    "\n",
    "\n",
    "# Evaluation function\n",
    "def black_box_function(W1, b1, W2, b2):\n",
    "    result = forward_prop(W1, b1, W2, b2, X_train)\n",
    "    y_train_encoded = np.array([y_train]) # np.eye(2)[y_train]\n",
    "    \n",
    "    return binary_cross_entropy(y_train_encoded, result)\n",
    "\n",
    "class CustomTask(TaskBase):\n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return 'Custom Task'\n",
    "\n",
    "    def evaluate(self, x: np.ndarray) -> np.ndarray:\n",
    "        y = np.zeros((len(x), 1))\n",
    "        print(x, '--------------------------------------------')\n",
    "        for ind in range(len(x)):\n",
    "            x_ind = x.iloc[ind].to_numpy()  # Convert Series to NumPy array\n",
    "            print(\"Evaluating with parameters:\")\n",
    "            print(\"x_ind:\", x_ind)\n",
    "            print('xind shape' , x_ind.shape)\n",
    "            y[ind] = black_box_function(x_ind[:16].reshape(4, 4), x_ind[16:20].reshape(4, 1), \n",
    "                                        x_ind[20:24].reshape(1, 4), x_ind[24:].reshape(1,1))\n",
    "        return y\n",
    "\n",
    "    \n",
    "    def get_search_space_params(self) -> List[Dict[str, Any]]:\n",
    "        params = [{'name': f'W1_{i}{j}', 'type': 'num', 'lb': -1, 'ub': 1} for i in range(4) for j in range(4)]\n",
    "        params.extend([{'name': f'b1_{i}', 'type': 'num', 'lb': -1, 'ub': 1} for i in range(4)])\n",
    "        params.extend([{'name': f'W2_{i}{j}', 'type': 'num', 'lb': -1, 'ub': 1} for i in range(1) for j in range(4)])\n",
    "        params.extend([{'name': f'b2_{i}', 'type': 'num', 'lb': -1, 'ub': 1} for i in range(1)])\n",
    "        return params\n",
    "\n",
    "    def get_parameter_names(self) -> List[str]:\n",
    "        params = []\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                params.append(f'W1_{i}{j}')\n",
    "        params.extend([f'b1_{i}' for i in range(4)])\n",
    "        for i in range(1):\n",
    "            for j in range(4):\n",
    "                params.append(f'W2_{i}{j}')\n",
    "        params.extend([f'b2_{i}' for i in range(1)])\n",
    "        return params\n",
    "\n",
    "# Creating task and optimization\n",
    "task = CustomTask()\n",
    "searchspace = task.get_search_space()\n",
    "\n",
    "epochs = 20\n",
    "bo_builder = BoBuilder(model_id='gp_hed', acq_opt_id='is', acq_func_id='ei', tr_id='basic')\n",
    "opt = bo_builder.build_bo(search_space=task.get_search_space(), n_init=50)\n",
    "\n",
    "# Optimization loop\n",
    "budget_eval = epochs\n",
    "# Inside the optimization loop\n",
    "for _ in range(budget_eval):\n",
    "    x_next = opt.suggest()\n",
    "    x_next_reshaped = np.array(x_next).reshape(1, -1)  # Reshape x_next to have shape (1, 25)\n",
    "    x_next_df = pd.DataFrame(x_next_reshaped, columns=task.get_parameter_names())\n",
    "    print(x_next_df.shape, 'xnextdf')\n",
    "    y_next = task.evaluate(x_next_df)\n",
    "    opt.observe(x_next, y_next)\n",
    "\n",
    "    \n",
    "# Printing best result\n",
    "print('Best parameters:', opt.best_x)\n",
    "print('Best loss:', opt.best_y)\n",
    "\n",
    "# Plotting version\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "y = opt.data_buffer.y.numpy()\n",
    "regret_y = np.min(np.cumsum(y, axis=1), axis=0)\n",
    "#plt.plot(np.arange(1,  budget_eval ), regret_y)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "d8e63b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5106061662116507\n"
     ]
    }
   ],
   "source": [
    "l1=np.array([-0.61717523  ,0.36990035,  0.49814788, -0.5190669,   0.51623886 ,-0.35973204\n",
    " , 0.1933757  , 0.67851127 , 0.19358574, -0.43996284, -0.62441788 , 0.3295846\n",
    ", -0.4392478 , -0.55685931 ,-0.26148563 , 0.62263243 , 0.84988384, -0.54001019\n",
    ", -0.4897618  ,-0.74264208, -0.32725616, -0.69292137 ,-0.18909265,  0.39931873\n",
    ",  0.57353324])\n",
    "\n",
    "par1,par2,par3,par4= l1[:16].reshape(4, 4), l1[16:20].reshape(4, 1), l1[20:24].reshape(1, 4), l1[24:].reshape(1,1)\n",
    "for_prop=(forward_prop(par1,par2,par3,par4, X_train))\n",
    "y_train_encoded = np.array([y_train]) # np.eye(2)[y_train]\n",
    "res = binary_cross_entropy( y_train_encoded, for_prop)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "44e0af21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>W1_00</th>\n",
       "      <th>W1_01</th>\n",
       "      <th>W1_02</th>\n",
       "      <th>W1_03</th>\n",
       "      <th>W1_10</th>\n",
       "      <th>W1_11</th>\n",
       "      <th>W1_12</th>\n",
       "      <th>W1_13</th>\n",
       "      <th>W1_20</th>\n",
       "      <th>W1_21</th>\n",
       "      <th>...</th>\n",
       "      <th>W1_33</th>\n",
       "      <th>b1_0</th>\n",
       "      <th>b1_1</th>\n",
       "      <th>b1_2</th>\n",
       "      <th>b1_3</th>\n",
       "      <th>W2_00</th>\n",
       "      <th>W2_01</th>\n",
       "      <th>W2_02</th>\n",
       "      <th>W2_03</th>\n",
       "      <th>b2_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.657061</td>\n",
       "      <td>0.293242</td>\n",
       "      <td>0.913671</td>\n",
       "      <td>0.571333</td>\n",
       "      <td>-0.36779</td>\n",
       "      <td>-0.80862</td>\n",
       "      <td>0.310766</td>\n",
       "      <td>-0.572527</td>\n",
       "      <td>0.146803</td>\n",
       "      <td>-0.797944</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033472</td>\n",
       "      <td>-0.243607</td>\n",
       "      <td>0.08836</td>\n",
       "      <td>0.141123</td>\n",
       "      <td>-0.38469</td>\n",
       "      <td>-0.803011</td>\n",
       "      <td>0.71257</td>\n",
       "      <td>-0.954861</td>\n",
       "      <td>-0.362605</td>\n",
       "      <td>-0.57053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      W1_00     W1_01     W1_02     W1_03    W1_10    W1_11     W1_12  \\\n",
       "0  0.657061  0.293242  0.913671  0.571333 -0.36779 -0.80862  0.310766   \n",
       "\n",
       "      W1_13     W1_20     W1_21  ...     W1_33      b1_0     b1_1      b1_2  \\\n",
       "0 -0.572527  0.146803 -0.797944  ...  0.033472 -0.243607  0.08836  0.141123   \n",
       "\n",
       "      b1_3     W2_00    W2_01     W2_02     W2_03     b2_0  \n",
       "0 -0.38469 -0.803011  0.71257 -0.954861 -0.362605 -0.57053  \n",
       "\n",
       "[1 rows x 25 columns]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_weights_and_biases = dict(opt.best_x)\n",
    "opt.best_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "dac74f8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 0.65706052,  0.29324227,  0.91367134,  0.5713332 , -0.36779022,\n",
       "         -0.80861977,  0.3107664 , -0.57252662,  0.14680325, -0.79794351,\n",
       "         -0.04675909, -0.91023797,  0.8867146 ,  0.1539183 ,  0.86388828,\n",
       "          0.03347209]]),\n",
       " 'b1': array([[-0.2436073 ,  0.08836023,  0.14112281, -0.38468983]]),\n",
       " 'W2': array([[-0.80301072,  0.71257026, -0.95486061, -0.36260549]]),\n",
       " 'b2': array([[-0.57052975]])}"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def weights_and_biases_matrix(df_pars):\n",
    "    # Group the columns by their prefixes\n",
    "    grouped_columns = df_pars.columns.to_series().groupby(lambda x: x.split('_')[0], sort=False)\n",
    "\n",
    "    # Use dictionary comprehension to create the result dictionary\n",
    "   # result_arrays = {prefix: np.vstack([df_pars[cols].columns, df_pars[cols].values])\n",
    "    #                 for prefix, cols in grouped_columns.groups.items()}\n",
    "    result_arrays = {prefix: np.array(df_pars[cols].values)\n",
    "                     for prefix, cols in grouped_columns.groups.items()}\n",
    "\n",
    "    return result_arrays\n",
    "\n",
    "    #return weights_set_list#W1, weights_W2\n",
    "\n",
    "weights_and_biases_results = (weights_and_biases_matrix(opt.best_x))\n",
    "(weights_and_biases_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "879f2ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287\n",
      "478\n",
      "accuracy = 0.600418410041841\n"
     ]
    }
   ],
   "source": [
    "def forward_prop_adj(W1, b1, W2, b2, X_data):\n",
    "    n_features = X_data.shape[1]\n",
    "    if W1.shape[1] != n_features:\n",
    "        W1_shape_0 = int(W1.size / n_features) \n",
    "        W1 = np.reshape(W1, (W1_shape_0 ,n_features))\n",
    "    if W2.shape[1] != W1.shape[0]:\n",
    "        W2_shape_0 = int(W2.size / W1.shape[0])\n",
    "        W2 = np.reshape(W2, (W2_shape_0, W1.shape[0]))\n",
    "    Z1 = np.dot(W1, X_data.T) + b1.T\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2.T\n",
    "    A2 = sigmoid(Z2).T \n",
    "    return A2\n",
    "\n",
    "def binarize_outcome(res_arr):\n",
    "    return np.where(res_arr<.5,0,1)\n",
    "\n",
    "def calculate_accuracy(actual_val, pred_val):\n",
    "    correct = np.sum(actual_val == pred_val)\n",
    "    total = len(actual_val)\n",
    "    accuracy = correct / total\n",
    "    print(correct)\n",
    "    print(total)\n",
    "    return accuracy\n",
    "\n",
    "preds_train_bo = forward_prop_adj(**weights_and_biases_results, X_data=X_train)\n",
    "binary_res = binarize_outcome(preds_train_bo)\n",
    "print('accuracy =', calculate_accuracy(np.array(y_train), binary_res.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "37907a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287\n",
      "478\n",
      "accuracy = 0.600418410041841\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print('accuracy =', calculate_accuracy(np.array(y_train), binary_res.flatten()))\n",
    "confusion_matrix = metrics.confusion_matrix(y_train, binary_res.flatten())\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "aabb8433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(binary_res.flatten())\n",
    "plt.plot(preds_train_bo)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a4a4e171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.40761507]\n",
      " [0.470208  ]\n",
      " [0.40554545]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.41248621]\n",
      " [0.44916972]\n",
      " [0.41230894]\n",
      " [0.40797339]\n",
      " [0.40860949]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.40916443]\n",
      " [0.470208  ]\n",
      " [0.40814292]\n",
      " [0.470208  ]\n",
      " [0.41204252]\n",
      " [0.40908308]\n",
      " [0.40793348]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.40881179]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.40520101]\n",
      " [0.40740696]\n",
      " [0.40725289]\n",
      " [0.40419544]\n",
      " [0.470208  ]\n",
      " [0.43576029]\n",
      " [0.470208  ]\n",
      " [0.40754705]\n",
      " [0.470208  ]\n",
      " [0.40927395]\n",
      " [0.40502724]\n",
      " [0.4110313 ]\n",
      " [0.470208  ]\n",
      " [0.4114367 ]\n",
      " [0.470208  ]\n",
      " [0.40688346]\n",
      " [0.40819908]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.40541291]\n",
      " [0.40687646]\n",
      " [0.470208  ]\n",
      " [0.40918978]\n",
      " [0.40778645]\n",
      " [0.40760307]\n",
      " [0.40805274]\n",
      " [0.40819044]\n",
      " [0.40841989]\n",
      " [0.40699564]\n",
      " [0.40805086]\n",
      " [0.4045623 ]\n",
      " [0.41065184]\n",
      " [0.470208  ]\n",
      " [0.41047419]\n",
      " [0.40886334]\n",
      " [0.4118736 ]\n",
      " [0.470208  ]\n",
      " [0.47128252]\n",
      " [0.470208  ]\n",
      " [0.40615556]\n",
      " [0.470208  ]\n",
      " [0.4085976 ]\n",
      " [0.40648548]\n",
      " [0.40860949]\n",
      " [0.40982046]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.40841288]\n",
      " [0.40712017]\n",
      " [0.470208  ]\n",
      " [0.40872423]\n",
      " [0.40673891]\n",
      " [0.4089808 ]\n",
      " [0.40967871]\n",
      " [0.40814816]\n",
      " [0.40687159]\n",
      " [0.470208  ]\n",
      " [0.40898197]\n",
      " [0.40461797]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.40565272]\n",
      " [0.40649281]\n",
      " [0.4063705 ]\n",
      " [0.41290307]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.40885469]\n",
      " [0.41196139]\n",
      " [0.470208  ]\n",
      " [0.40770986]\n",
      " [0.470208  ]\n",
      " [0.40589037]\n",
      " [0.470208  ]\n",
      " [0.40898524]\n",
      " [0.470208  ]\n",
      " [0.40846929]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.40907293]\n",
      " [0.41288998]\n",
      " [0.41188143]\n",
      " [0.40750912]\n",
      " [0.470208  ]\n",
      " [0.40276958]\n",
      " [0.40224069]\n",
      " [0.470208  ]\n",
      " [0.41139273]\n",
      " [0.40671494]\n",
      " [0.40829643]\n",
      " [0.41563809]\n",
      " [0.470208  ]\n",
      " [0.40694951]\n",
      " [0.41200002]\n",
      " [0.470208  ]\n",
      " [0.40904913]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.41130464]\n",
      " [0.470208  ]\n",
      " [0.40839523]\n",
      " [0.470208  ]\n",
      " [0.4098537 ]\n",
      " [0.40553667]\n",
      " [0.470208  ]\n",
      " [0.41143227]\n",
      " [0.40774455]\n",
      " [0.40644831]\n",
      " [0.40620135]\n",
      " [0.470208  ]\n",
      " [0.41491082]\n",
      " [0.470208  ]\n",
      " [0.40655015]\n",
      " [0.4759974 ]\n",
      " [0.41026961]\n",
      " [0.41063368]\n",
      " [0.470208  ]\n",
      " [0.410937  ]\n",
      " [0.41972416]\n",
      " [0.40910204]\n",
      " [0.470208  ]\n",
      " [0.40792483]\n",
      " [0.40406381]\n",
      " [0.40926658]\n",
      " [0.470208  ]\n",
      " [0.40667182]\n",
      " [0.470208  ]\n",
      " [0.40841872]\n",
      " [0.470208  ]\n",
      " [0.41165562]\n",
      " [0.40767981]\n",
      " [0.470208  ]\n",
      " [0.41197365]\n",
      " [0.470208  ]\n",
      " [0.40889287]\n",
      " [0.470208  ]\n",
      " [0.40747903]\n",
      " [0.41026286]\n",
      " [0.470208  ]\n",
      " [0.40783502]\n",
      " [0.40934892]\n",
      " [0.40830474]\n",
      " [0.40774919]\n",
      " [0.40465046]\n",
      " [0.40274831]\n",
      " [0.41182834]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.40886941]\n",
      " [0.40942354]\n",
      " [0.40703859]\n",
      " [0.40858358]\n",
      " [0.40583039]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.40685217]\n",
      " [0.470208  ]\n",
      " [0.40687159]\n",
      " [0.40712998]\n",
      " [0.41080857]\n",
      " [0.40767246]\n",
      " [0.40589138]\n",
      " [0.40500443]\n",
      " [0.40606711]\n",
      " [0.470208  ]\n",
      " [0.40510313]\n",
      " [0.40879109]\n",
      " [0.470208  ]\n",
      " [0.40715449]\n",
      " [0.47290548]\n",
      " [0.40840359]\n",
      " [0.40814302]\n",
      " [0.470208  ]\n",
      " [0.40899623]\n",
      " [0.41511027]\n",
      " [0.40724096]\n",
      " [0.470208  ]\n",
      " [0.40752653]\n",
      " [0.41237599]\n",
      " [0.40575779]\n",
      " [0.40861909]\n",
      " [0.40914103]\n",
      " [0.40479024]\n",
      " [0.40860949]\n",
      " [0.40765695]\n",
      " [0.40739378]\n",
      " [0.470208  ]\n",
      " [0.40867437]\n",
      " [0.470208  ]\n",
      " [0.40620135]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.40345697]\n",
      " [0.40918978]\n",
      " [0.40762789]\n",
      " [0.40628226]\n",
      " [0.4075454 ]\n",
      " [0.40727423]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.40905958]\n",
      " [0.40885469]\n",
      " [0.470208  ]\n",
      " [0.40446027]\n",
      " [0.470208  ]\n",
      " [0.40876481]\n",
      " [0.41223683]\n",
      " [0.470208  ]\n",
      " [0.40859059]\n",
      " [0.470208  ]\n",
      " [0.40884791]\n",
      " [0.470208  ]\n",
      " [0.40879636]\n",
      " [0.40605045]\n",
      " [0.41245779]\n",
      " [0.41219409]\n",
      " [0.470208  ]\n",
      " [0.41179246]\n",
      " [0.40735643]\n",
      " [0.40319296]\n",
      " [0.40832093]\n",
      " [0.40886334]\n",
      " [0.40702629]\n",
      " [0.470208  ]\n",
      " [0.40659762]\n",
      " [0.40389992]\n",
      " [0.40578259]\n",
      " [0.470208  ]\n",
      " [0.41216341]\n",
      " [0.40649117]\n",
      " [0.470208  ]\n",
      " [0.40790815]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.40740242]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.40807082]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.41720993]\n",
      " [0.470208  ]\n",
      " [0.41299767]\n",
      " [0.40602956]\n",
      " [0.40792483]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.40810513]\n",
      " [0.40751976]\n",
      " [0.40402209]\n",
      " [0.470208  ]\n",
      " [0.40767981]\n",
      " [0.40156003]\n",
      " [0.4101387 ]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.40861916]\n",
      " [0.470208  ]\n",
      " [0.40880714]\n",
      " [0.470208  ]\n",
      " [0.4084561 ]\n",
      " [0.40648548]\n",
      " [0.470208  ]\n",
      " [0.41466563]\n",
      " [0.40673028]\n",
      " [0.40578818]\n",
      " [0.470208  ]\n",
      " [0.40710993]\n",
      " [0.40994724]\n",
      " [0.40733606]\n",
      " [0.40802003]\n",
      " [0.41272897]\n",
      " [0.40847664]\n",
      " [0.470208  ]\n",
      " [0.41450695]\n",
      " [0.40419544]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.40608761]\n",
      " [0.470208  ]\n",
      " [0.40499011]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.40845049]\n",
      " [0.40660133]\n",
      " [0.40714886]\n",
      " [0.470208  ]\n",
      " [0.40779205]\n",
      " [0.40839779]\n",
      " [0.470208  ]\n",
      " [0.40475043]\n",
      " [0.470208  ]\n",
      " [0.40906693]\n",
      " [0.41052975]\n",
      " [0.40443281]\n",
      " [0.40742879]\n",
      " [0.470208  ]\n",
      " [0.40885469]\n",
      " [0.470208  ]\n",
      " [0.40725429]\n",
      " [0.40868442]\n",
      " [0.470208  ]\n",
      " [0.40776066]\n",
      " [0.40760849]\n",
      " [0.40841186]\n",
      " [0.40774919]\n",
      " [0.40657522]\n",
      " [0.470208  ]\n",
      " [0.41171   ]\n",
      " [0.40841989]\n",
      " [0.40924662]\n",
      " [0.41166382]\n",
      " [0.40939953]\n",
      " [0.4122732 ]\n",
      " [0.470208  ]\n",
      " [0.47925384]\n",
      " [0.40714886]\n",
      " [0.470208  ]\n",
      " [0.41064628]\n",
      " [0.40802143]\n",
      " [0.470208  ]\n",
      " [0.40534355]\n",
      " [0.40818483]\n",
      " [0.40765368]\n",
      " [0.470208  ]\n",
      " [0.40673891]\n",
      " [0.470208  ]\n",
      " [0.47303081]\n",
      " [0.40809639]\n",
      " [0.40907326]\n",
      " [0.40751836]\n",
      " [0.470208  ]\n",
      " [0.40963943]\n",
      " [0.470208  ]\n",
      " [0.40896782]\n",
      " [0.470208  ]\n",
      " [0.40827244]\n",
      " [0.41150984]\n",
      " [0.40748367]\n",
      " [0.40815969]\n",
      " [0.470208  ]\n",
      " [0.4071873 ]\n",
      " [0.470208  ]\n",
      " [0.40855273]\n",
      " [0.40562136]\n",
      " [0.40200691]\n",
      " [0.470208  ]\n",
      " [0.40761642]\n",
      " [0.40750811]\n",
      " [0.40599593]\n",
      " [0.40964989]\n",
      " [0.40958349]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.40577938]\n",
      " [0.40858662]\n",
      " [0.4075971 ]\n",
      " [0.40386332]\n",
      " [0.470208  ]\n",
      " [0.40796781]\n",
      " [0.40857984]\n",
      " [0.41072569]\n",
      " [0.40669681]\n",
      " [0.40522435]\n",
      " [0.40922519]\n",
      " [0.470208  ]\n",
      " [0.4108202 ]\n",
      " [0.40522435]\n",
      " [0.4053005 ]\n",
      " [0.47219077]\n",
      " [0.470208  ]\n",
      " [0.40777537]\n",
      " [0.470208  ]\n",
      " [0.40747574]\n",
      " [0.40740242]\n",
      " [0.470208  ]\n",
      " [0.40901025]\n",
      " [0.470208  ]\n",
      " [0.40703812]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.40926205]\n",
      " [0.40379829]\n",
      " [0.40202115]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.40832326]\n",
      " [0.470208  ]\n",
      " [0.41213322]\n",
      " [0.470208  ]\n",
      " [0.40780524]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.40828975]\n",
      " [0.44682319]\n",
      " [0.40761549]\n",
      " [0.40683158]\n",
      " [0.40833645]\n",
      " [0.40554545]\n",
      " [0.40886215]\n",
      " [0.40804385]\n",
      " [0.470208  ]\n",
      " [0.40721357]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.40858334]\n",
      " [0.470208  ]\n",
      " [0.40678454]\n",
      " [0.4083438 ]\n",
      " [0.470208  ]\n",
      " [0.470208  ]\n",
      " [0.40712834]\n",
      " [0.40916194]\n",
      " [0.40713698]\n",
      " [0.41267292]\n",
      " [0.41288538]\n",
      " [0.40791806]\n",
      " [0.470208  ]\n",
      " [0.40997302]\n",
      " [0.40628498]\n",
      " [0.40846475]\n",
      " [0.40712998]\n",
      " [0.40858217]\n",
      " [0.41103841]\n",
      " [0.40846475]\n",
      " [0.40486819]\n",
      " [0.40828038]\n",
      " [0.40920719]\n",
      " [0.470208  ]\n",
      " [0.40825557]\n",
      " [0.4083438 ]\n",
      " [0.470208  ]\n",
      " [0.40770324]\n",
      " [0.40699564]]\n"
     ]
    }
   ],
   "source": [
    "print(preds_train_bo)\n",
    "plt.hist(preds_train_bo)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ea1a3e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of predictors: (625, 4)\n",
      "Shape of target: (625,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the Eggholder function\n",
    "def eggholder(x, y):\n",
    "    return (-(y + 47) * np.sin(np.sqrt(np.abs(x / 2 + (y + 47)))) - x * np.sin(np.sqrt(np.abs(x - (y + 47)))))\n",
    "\n",
    "# Define the range of x and y values\n",
    "x_min, x_max = -512, 512\n",
    "y_min, y_max = -512, 512\n",
    "\n",
    "# Define the number of points per dimension\n",
    "num_points_per_dimension =25\n",
    "\n",
    "# Generate X1, X2, X3, and X4 coordinates with the specified number of points\n",
    "x = np.linspace(x_min, x_max, num_points_per_dimension)\n",
    "y = np.linspace(y_min, y_max, num_points_per_dimension)\n",
    "\n",
    "# Generate the meshgrid\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# Calculate Z values using the Eggholder function\n",
    "Z_eggholder = eggholder(X, Y)\n",
    "\n",
    "# Flatten X, Y, and Z to create the predictors\n",
    "X1_flat = X.flatten()\n",
    "X2_flat = Y.flatten()\n",
    "X3_flat = (X ** 2).flatten()\n",
    "X4_flat = (Y ** 2).flatten()\n",
    "\n",
    "# Combine X1, X2, X3, and X4 to create the predictors\n",
    "X = np.column_stack((X1_flat, X2_flat, X3_flat, X4_flat))\n",
    "\n",
    "# Binarize the Z_eggholder values to create the binary target variable\n",
    "threshold = np.mean(Z_eggholder)\n",
    "y = (Z_eggholder.flatten() > threshold).astype(int)\n",
    "\n",
    "# Display the shapes of predictors and target\n",
    "print(\"Shape of predictors:\", X.shape)\n",
    "print(\"Shape of target:\", y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "63d79da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ydg88\\AppData\\Local\\Temp\\ipykernel_6264\\2329022958.py:35: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.sum(Y * np.log(A2) + (1 - Y) * np.log(1 - A2)) / m\n",
      "C:\\Users\\ydg88\\AppData\\Local\\Temp\\ipykernel_6264\\2329022958.py:35: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.sum(Y * np.log(A2) + (1 - Y) * np.log(1 - A2)) / m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: nan\n",
      "Output after iteration 0: [[1.         1.         1.         0.96755257 0.9974931  1.\n",
      "  1.         1.         1.         1.         1.         0.96340833\n",
      "  0.99896578 0.99999991 1.         1.         0.7917838  1.\n",
      "  1.         0.65426579 1.         1.         0.99999948 1.\n",
      "  1.         1.         1.         0.96390026 0.99998404 1.\n",
      "  0.99999999 1.         0.99998957 0.9993343  0.99999941 1.\n",
      "  1.         1.         0.99999995 0.99999996 1.         1.\n",
      "  0.99999998 1.         1.         1.         1.         0.99999995\n",
      "  1.         0.93965784 1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         0.99999294\n",
      "  1.         1.         0.99999996 0.99999995 1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         0.99986656 1.         1.         0.99998072 0.99880273\n",
      "  1.         0.99999998 0.99999948 1.         1.         1.\n",
      "  1.         1.         1.         0.99982811 0.69556799 1.\n",
      "  1.         0.999769   1.         1.         1.         1.\n",
      "  1.         1.         1.         0.9999985  0.99999998 1.\n",
      "  0.99999375 1.         0.99999942 1.         1.         1.\n",
      "  0.9401503  1.         0.99999994 1.         1.         0.99684789\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  0.99999992 1.         1.         1.         1.         1.\n",
      "  1.         1.         0.81893717 1.         1.         0.99998099\n",
      "  0.984342   1.         0.94018668 1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         0.99999951 0.92872779 0.99980016 0.99999866 1.\n",
      "  1.         0.99997122 1.         1.         0.99999992 0.99999888\n",
      "  1.         1.         1.         1.         1.         0.99999999\n",
      "  0.92802136 1.         0.99999998 1.         0.99514592 1.\n",
      "  0.98371024 1.         1.         1.         0.98543485 1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         0.99849568 0.99999932 1.         1.\n",
      "  0.99999993 1.         0.9999895  0.99888733 0.99999848 1.\n",
      "  1.         0.99999999 1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         0.99283324 1.\n",
      "  0.99999999 1.         1.         1.         1.         1.\n",
      "  1.         1.         0.99999991 0.99745107 1.         0.99987851\n",
      "  0.99895117 1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         0.66664965 1.         1.\n",
      "  0.99979591 0.99770986 1.         1.         0.9863658  1.\n",
      "  0.99998361 1.         0.99495109 0.99488004 0.99995547 1.\n",
      "  1.         1.         0.9922869  1.         0.99934817 1.\n",
      "  1.         1.         0.99999998 1.         1.         1.\n",
      "  0.78733133 1.         1.         1.         1.         1.\n",
      "  1.         0.99999998 1.         1.         1.         1.\n",
      "  0.96559459 1.         0.99995835 0.99999991 1.         1.\n",
      "  1.         1.         1.         1.         1.         0.99741023\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         0.99936051 1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         0.99999992\n",
      "  0.99995922 1.         1.         0.99999955 1.         1.\n",
      "  1.         0.99999993 1.         1.         1.         1.\n",
      "  1.         1.         1.         0.99999924 1.         1.\n",
      "  0.69407884 0.99999993 0.99981059 1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         0.99999994 0.99999989 0.99999789\n",
      "  1.         0.65546637 0.99999056 1.         1.         1.\n",
      "  1.         0.99998245 1.         1.         1.         1.\n",
      "  1.         1.         0.99852699 1.         1.         1.\n",
      "  1.         0.99999999 1.         1.         1.         1.\n",
      "  0.9952133  1.         0.79293922 1.         0.99999063 0.99980655\n",
      "  1.         0.82482124 0.99689757 1.         1.         1.\n",
      "  1.         1.         1.         0.99999362 1.         1.\n",
      "  0.99999995 1.         1.         1.         1.         1.\n",
      "  1.         1.         0.54403992 1.         1.         1.\n",
      "  0.99999999 0.99848705 0.99997169 1.         0.99889511 0.93654636\n",
      "  1.         1.         1.         1.         1.         0.99986749\n",
      "  0.99998359 1.         1.         1.         1.         1.\n",
      "  1.         0.99999864 0.99999995 1.        ]]\n",
      "Cost after iteration 100: 289.69245779422783\n",
      "Output after iteration 100: [[0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703]]\n",
      "Cost after iteration 200: 289.69245779422783\n",
      "Output after iteration 200: [[0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ydg88\\AppData\\Local\\Temp\\ipykernel_6264\\2329022958.py:8: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 300: 289.69245779422783\n",
      "Output after iteration 300: [[0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703]]\n",
      "Cost after iteration 400: 289.69245779422783\n",
      "Output after iteration 400: [[0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703]]\n",
      "Cost after iteration 500: 289.69245779422783\n",
      "Output after iteration 500: [[0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703]]\n",
      "Cost after iteration 600: 289.69245779422783\n",
      "Output after iteration 600: [[0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 700: 289.69245779422783\n",
      "Output after iteration 700: [[0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703]]\n",
      "Cost after iteration 800: 289.69245779422783\n",
      "Output after iteration 800: [[0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703]]\n",
      "Cost after iteration 900: 289.69245779422783\n",
      "Output after iteration 900: [[0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703]]\n",
      "Final predictions:\n",
      "[[0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def binarize_outcome(res_arr):\n",
    "    return np.where(res_arr<.5,0,1)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def initialize_parameters(input_size, hidden_size, output_size):\n",
    "    np.random.seed(0)\n",
    "    W1 = np.random.randn(hidden_size, input_size) * 0.01\n",
    "    b1 = np.zeros((hidden_size, 1))\n",
    "    W2 = np.random.randn(output_size, hidden_size) * 0.01\n",
    "    b2 = np.zeros((output_size, 1))\n",
    "    return {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    W1, b1, W2, b2 = parameters['W1'], parameters['b1'], parameters['W2'], parameters['b2']\n",
    "    Z1 = np.dot(W1, X.T) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    cache = {'Z1': Z1, 'A1': A1, 'Z2': Z2, 'A2': A2}\n",
    "    return A2, cache\n",
    "    \n",
    "def compute_cost(A2, Y):\n",
    "    m = 1\n",
    "    cost = -np.sum(Y * np.log(A2) + (1 - Y) * np.log(1 - A2)) / m\n",
    "    return np.squeeze(cost)\n",
    "\n",
    "def backward_propagation(X, Y, parameters, cache):\n",
    "    m = X.shape[1]\n",
    "    A1, A2 = cache['A1'], cache['A2']\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = np.dot(dZ2, A1.T) / m\n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True) / m\n",
    "    dZ1 = np.dot(parameters['W2'].T, dZ2) * relu_derivative(A1)\n",
    "    dW1 = np.dot(dZ1, X) / m\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True) / m\n",
    "    return {'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2}\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    parameters['W1'] -= learning_rate * grads['dW1']\n",
    "    parameters['b1'] -= learning_rate * grads['db1']\n",
    "    parameters['W2'] -= learning_rate * grads['dW2']\n",
    "    parameters['b2'] -= learning_rate * grads['db2']\n",
    "    return parameters\n",
    "\n",
    "def train_neural_network(X, Y, input_size, hidden_size, output_size, num_iterations=50, learning_rate=0.1):\n",
    "    parameters = initialize_parameters(input_size, hidden_size, output_size)\n",
    "    for i in range(num_iterations):\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        \n",
    "        cost = compute_cost(A2, Y)\n",
    "        grads = backward_propagation(X, Y, parameters, cache)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        if i % 100 == 0:\n",
    "            print(f'Cost after iteration {i}: {cost}')\n",
    "            print(f'Output after iteration {i}: {A2}')\n",
    "    return parameters, A2  # Return the trained parameters and final predictions\n",
    "\n",
    "# Example usage:\n",
    "# Generate random input and output data\n",
    "\n",
    "\n",
    "\n",
    "# Train the neural network\n",
    "input_size = 4\n",
    "hidden_size = 4\n",
    "output_size = 1\n",
    "trained_parameters, final_predictions = train_neural_network(X_train, np.array(y_train), input_size, hidden_size, output_size, num_iterations=1000, learning_rate=0.01)\n",
    "\n",
    "print(\"Final predictions:\")\n",
    "print(final_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "79e87aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ydg88\\AppData\\Local\\Temp\\ipykernel_6264\\2329022958.py:35: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.sum(Y * np.log(A2) + (1 - Y) * np.log(1 - A2)) / m\n",
      "C:\\Users\\ydg88\\AppData\\Local\\Temp\\ipykernel_6264\\2329022958.py:35: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.sum(Y * np.log(A2) + (1 - Y) * np.log(1 - A2)) / m\n",
      "C:\\Users\\ydg88\\AppData\\Local\\Temp\\ipykernel_6264\\2329022958.py:8: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: nan\n",
      "Output after iteration 0: [[1.         1.         1.         0.96755257 0.9974931  1.\n",
      "  1.         1.         1.         1.         1.         0.96340833\n",
      "  0.99896578 0.99999991 1.         1.         0.7917838  1.\n",
      "  1.         0.65426579 1.         1.         0.99999948 1.\n",
      "  1.         1.         1.         0.96390026 0.99998404 1.\n",
      "  0.99999999 1.         0.99998957 0.9993343  0.99999941 1.\n",
      "  1.         1.         0.99999995 0.99999996 1.         1.\n",
      "  0.99999998 1.         1.         1.         1.         0.99999995\n",
      "  1.         0.93965784 1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         0.99999294\n",
      "  1.         1.         0.99999996 0.99999995 1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         0.99986656 1.         1.         0.99998072 0.99880273\n",
      "  1.         0.99999998 0.99999948 1.         1.         1.\n",
      "  1.         1.         1.         0.99982811 0.69556799 1.\n",
      "  1.         0.999769   1.         1.         1.         1.\n",
      "  1.         1.         1.         0.9999985  0.99999998 1.\n",
      "  0.99999375 1.         0.99999942 1.         1.         1.\n",
      "  0.9401503  1.         0.99999994 1.         1.         0.99684789\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  0.99999992 1.         1.         1.         1.         1.\n",
      "  1.         1.         0.81893717 1.         1.         0.99998099\n",
      "  0.984342   1.         0.94018668 1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         0.99999951 0.92872779 0.99980016 0.99999866 1.\n",
      "  1.         0.99997122 1.         1.         0.99999992 0.99999888\n",
      "  1.         1.         1.         1.         1.         0.99999999\n",
      "  0.92802136 1.         0.99999998 1.         0.99514592 1.\n",
      "  0.98371024 1.         1.         1.         0.98543485 1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         0.99849568 0.99999932 1.         1.\n",
      "  0.99999993 1.         0.9999895  0.99888733 0.99999848 1.\n",
      "  1.         0.99999999 1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         0.99283324 1.\n",
      "  0.99999999 1.         1.         1.         1.         1.\n",
      "  1.         1.         0.99999991 0.99745107 1.         0.99987851\n",
      "  0.99895117 1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         0.66664965 1.         1.\n",
      "  0.99979591 0.99770986 1.         1.         0.9863658  1.\n",
      "  0.99998361 1.         0.99495109 0.99488004 0.99995547 1.\n",
      "  1.         1.         0.9922869  1.         0.99934817 1.\n",
      "  1.         1.         0.99999998 1.         1.         1.\n",
      "  0.78733133 1.         1.         1.         1.         1.\n",
      "  1.         0.99999998 1.         1.         1.         1.\n",
      "  0.96559459 1.         0.99995835 0.99999991 1.         1.\n",
      "  1.         1.         1.         1.         1.         0.99741023\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         0.99936051 1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         0.99999992\n",
      "  0.99995922 1.         1.         0.99999955 1.         1.\n",
      "  1.         0.99999993 1.         1.         1.         1.\n",
      "  1.         1.         1.         0.99999924 1.         1.\n",
      "  0.69407884 0.99999993 0.99981059 1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         0.99999994 0.99999989 0.99999789\n",
      "  1.         0.65546637 0.99999056 1.         1.         1.\n",
      "  1.         0.99998245 1.         1.         1.         1.\n",
      "  1.         1.         0.99852699 1.         1.         1.\n",
      "  1.         0.99999999 1.         1.         1.         1.\n",
      "  0.9952133  1.         0.79293922 1.         0.99999063 0.99980655\n",
      "  1.         0.82482124 0.99689757 1.         1.         1.\n",
      "  1.         1.         1.         0.99999362 1.         1.\n",
      "  0.99999995 1.         1.         1.         1.         1.\n",
      "  1.         1.         0.54403992 1.         1.         1.\n",
      "  0.99999999 0.99848705 0.99997169 1.         0.99889511 0.93654636\n",
      "  1.         1.         1.         1.         1.         0.99986749\n",
      "  0.99998359 1.         1.         1.         1.         1.\n",
      "  1.         0.99999864 0.99999995 1.        ]]\n",
      "Cost after iteration 100: 289.69245779422783\n",
      "Output after iteration 100: [[0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703]]\n",
      "Cost after iteration 200: 289.69245779422783\n",
      "Output after iteration 200: [[0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703]]\n",
      "Cost after iteration 300: 289.69245779422783\n",
      "Output after iteration 300: [[0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703]]\n",
      "Cost after iteration 400: 289.69245779422783\n",
      "Output after iteration 400: [[0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703]]\n",
      "Cost after iteration 500: 289.69245779422783\n",
      "Output after iteration 500: [[0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 600: 289.69245779422783\n",
      "Output after iteration 600: [[0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703]]\n",
      "Cost after iteration 700: 289.69245779422783\n",
      "Output after iteration 700: [[0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703]]\n",
      "Cost after iteration 800: 289.69245779422783\n",
      "Output after iteration 800: [[0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703]]\n",
      "Cost after iteration 900: 289.69245779422783\n",
      "Output after iteration 900: [[0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703]]\n",
      "Final predictions on training data:\n",
      "[[0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703]]\n",
      "Final predictions on test data:\n",
      "[[0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703 0.50717703 0.50717703 0.50717703\n",
      "  0.50717703 0.50717703 0.50717703]]\n"
     ]
    }
   ],
   "source": [
    "trained_parameters, final_predictions_train = train_neural_network(X_train, np.array(y_train), input_size, hidden_size, output_size, num_iterations=1000, learning_rate=0.01)\n",
    "\n",
    "# Now, use the trained parameters to make predictions on the test data\n",
    "final_predictions_test, _ = forward_propagation(X_test, trained_parameters)\n",
    "\n",
    "print(\"Final predictions on training data:\")\n",
    "print(final_predictions_train)\n",
    "\n",
    "print(\"Final predictions on test data:\")\n",
    "print(final_predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "de3e2c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212\n",
      "418\n",
      "0.507177033492823\n",
      "104\n",
      "207\n",
      "0.5024154589371981\n"
     ]
    }
   ],
   "source": [
    "def binarize_outcome(res_arr):\n",
    "    return np.where(res_arr<.5,0,1)\n",
    "\n",
    "def calculate_accuracy(actual_val, pred_val):\n",
    "    correct = np.sum(actual_val == pred_val)\n",
    "    total = len(actual_val)\n",
    "    accuracy = correct / total\n",
    "    print(correct)\n",
    "    print(total)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "bin_out_backprop_train=(binarize_outcome(final_predictions_train[-1]))\n",
    "print(calculate_accuracy(bin_out_backprop_train, y_train))\n",
    "bin_out_backprop_test=(binarize_outcome(final_predictions_test[-1]))\n",
    "print(calculate_accuracy(bin_out_backprop_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b62d316a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "       1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,\n",
       "       0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "43bfc8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([array([[ 0.88596038, -1.00956549,  1.22039933,  1.42074376],\n",
      "       [ 0.52220002, -0.5955235 ,  0.71281002,  0.82938996],\n",
      "       [-0.04633879,  0.03022445,  0.08010684,  0.0266258 ],\n",
      "       [ 0.12226227, -0.00692515,  1.2344951 , -4.40113791]]), array([[-0.1452472 ],\n",
      "       [-0.08222628],\n",
      "       [-0.01918814],\n",
      "       [ 0.10650999]]), array([[ 1.64631059,  0.96568291, -0.08349684, -2.36681665]]), array([[-0.56869235]])])\n"
     ]
    }
   ],
   "source": [
    "print((trained_parameters.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ca27726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.concatenate(list(weights_and_biases_results.values()), axis = None),\n",
    "np.concatenate(list(trained_parameters.values()), axis=None))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f129bb13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.63615757,  0.02715206, -0.14785868,  0.13741609,  0.24334712,\n",
       "        0.31646704, -0.55240617,  0.54538903,  0.30573323, -0.1200694 ,\n",
       "       -0.90739278,  0.91180403, -0.3979356 ,  0.36028264,  0.43749272,\n",
       "       -0.65571392,  0.15265377, -0.35503987, -0.91208553,  0.04449118,\n",
       "       -0.63336928,  0.93814317,  0.44029527, -0.31406779, -0.11930932])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate(list(weights_and_biases_results.values()), axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "8cf16f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(final_predictions[-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "1f3fcb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         1.         0.30887158 0.        ]\n",
      " [1.         0.         0.19577783 0.07690368]\n",
      " [0.         1.         0.54762503 0.01571255]\n",
      " ...\n",
      " [1.         0.         0.50992712 0.26252652]\n",
      " [0.         1.         0.40939935 0.04006213]\n",
      " [0.         1.         0.40939935 0.01541158]]\n",
      "[0.5488135039273248, 0.7151893663724195, 0.6027633760716439, 0.5448831829968969] x_sample\n",
      "0.399581589958159 performance\n",
      "[0.4236547993389047, 0.6458941130666561, 0.4375872112626925, 0.8917730007820798] x_sample\n",
      "0.38493723849372385 performance\n",
      "[0.9636627605010293, 0.3834415188257777, 0.7917250380826646, 0.5288949197529045] x_sample\n",
      "0.39330543933054396 performance\n",
      "[0.5680445610939323, 0.925596638292661, 0.07103605819788694, 0.08712929970154071] x_sample\n",
      "0.399581589958159 performance\n",
      "[0.02021839744032572, 0.832619845547938, 0.7781567509498505, 0.8700121482468192] x_sample\n",
      "0.2824267782426778 performance\n",
      "[0.978618342232764, 0.7991585642167236, 0.46147936225293185, 0.7805291762864555] x_sample\n",
      "0.399581589958159 performance\n",
      "[0.11827442586893322, 0.6399210213275238, 0.1433532874090464, 0.9446689170495839] x_sample\n",
      "0.2280334728033473 performance\n",
      "[0.5218483217500717, 0.4146619399905236, 0.26455561210462697, 0.7742336894342167] x_sample\n",
      "0.47489539748953974 performance\n",
      "[0.45615033221654855, 0.5684339488686485, 0.018789800436355142, 0.6176354970758771] x_sample\n",
      "0.3179916317991632 performance\n",
      "[0.6120957227224214, 0.6169339968747569, 0.9437480785146242, 0.6818202991034834] x_sample\n",
      "0.399581589958159 performance\n",
      "[0.359507900573786, 0.43703195379934145, 0.6976311959272649, 0.06022547162926983] x_sample\n",
      "0.3619246861924686 performance\n",
      "[0.6667667154456677, 0.6706378696181594, 0.2103825610738409, 0.1289262976548533] x_sample\n",
      "0.399581589958159 performance\n",
      "[0.31542835092418386, 0.3637107709426226, 0.5701967704178796, 0.43860151346232035] x_sample\n",
      "0.3891213389121339 performance\n",
      "[0.9883738380592262, 0.10204481074802807, 0.2088767560948347, 0.16130951788499626] x_sample\n",
      "0.7928870292887029 performance\n",
      "[0.6531083254653984, 0.2532916025397821, 0.4663107728563063, 0.24442559200160274] x_sample\n",
      "0.6841004184100419 performance\n",
      "[0.15896958364551972, 0.11037514116430513, 0.6563295894652734, 0.1381829513486138] x_sample\n",
      "0.5669456066945606 performance\n",
      "[0.1965823616800535, 0.3687251706609641, 0.8209932298479351, 0.09710127579306127] x_sample\n",
      "0.3054393305439331 performance\n",
      "[0.8379449074988039, 0.09609840789396307, 0.9764594650133958, 0.4686512016477016] x_sample\n",
      "0.5962343096234309 performance\n",
      "[0.9767610881903371, 0.604845519745046, 0.7392635793983017, 0.039187792254320675] x_sample\n",
      "0.399581589958159 performance\n",
      "[0.2828069625764096, 0.1201965612131689, 0.29614019752214493, 0.11872771895424405] x_sample\n",
      "0.6171548117154811 performance\n",
      "[0.317983179393976, 0.41426299451466997, 0.06414749634878436, 0.6924721193700198] x_sample\n",
      "0.5899581589958159 performance\n",
      "[0.5666014542065752, 0.2653894909394454, 0.5232480534666997, 0.09394051075844168] x_sample\n",
      "0.6401673640167364 performance\n",
      "[0.5759464955561793, 0.9292961975762141, 0.31856895245132366, 0.6674103799636817] x_sample\n",
      "0.399581589958159 performance\n",
      "[0.13179786240439217, 0.7163272041185655, 0.2894060929472011, 0.18319136200711683] x_sample\n",
      "0.20711297071129708 performance\n",
      "[0.5865129348100832, 0.020107546187493552, 0.8289400292173631, 0.004695476192547066] x_sample\n",
      "0.7154811715481172 performance\n",
      "[0.6778165367962301, 0.27000797319216485, 0.7351940221225949, 0.9621885451174382] x_sample\n",
      "0.497907949790795 performance\n",
      "[0.24875314351995803, 0.5761573344178369, 0.592041931271839, 0.5722519057908734] x_sample\n",
      "0.3284518828451883 performance\n",
      "[0.2230816326406183, 0.952749011516985, 0.44712537861762736, 0.8464086724711278] x_sample\n",
      "0.29916317991631797 performance\n",
      "[0.6994792753175043, 0.29743695085513366, 0.8137978197024772, 0.39650574084698464] x_sample\n",
      "0.4476987447698745 performance\n",
      "[0.8811031971111616, 0.5812728726358587, 0.8817353618548528, 0.6925315900777659] x_sample\n",
      "0.399581589958159 performance\n",
      "[0.7252542798196405, 0.5013243819267023, 0.9560836347232239, 0.6439901992296374] x_sample\n",
      "0.399581589958159 performance\n",
      "[0.4238550485581797, 0.6063932141279244, 0.019193198309333526, 0.30157481667454933] x_sample\n",
      "0.2405857740585774 performance\n",
      "[0.660173537492685, 0.29007760721044407, 0.6180154289988415, 0.42876870094576613] x_sample\n",
      "0.5502092050209205 performance\n",
      "[0.13547406422245023, 0.29828232595603077, 0.5699649107012649, 0.5908727612481732] x_sample\n",
      "0.4205020920502092 performance\n",
      "[0.5743252488495788, 0.6532008198571336, 0.6521032700016889, 0.43141843543397396] x_sample\n",
      "0.399581589958159 performance\n",
      "[0.896546595851063, 0.36756187004789653, 0.4358649252656268, 0.8919233550156721] x_sample\n",
      "0.48326359832635984 performance\n",
      "[0.8061939890460857, 0.7038885835403663, 0.10022688731230112, 0.9194826137446735] x_sample\n",
      "0.399581589958159 performance\n",
      "[0.7142412995491114, 0.9988470065678665, 0.14944830465799375, 0.8681260573682142] x_sample\n",
      "0.399581589958159 performance\n",
      "[0.16249293467637482, 0.6155595642838442, 0.12381998284944151, 0.8480082293222344] x_sample\n",
      "0.2280334728033473 performance\n",
      "[0.8073189587250107, 0.5691007386145933, 0.40718329722599966, 0.06916699545513805] x_sample\n",
      "0.399581589958159 performance\n",
      "[0.6974287731445636, 0.45354268267806885, 0.7220555994703479, 0.8663823259286292] x_sample\n",
      "0.39330543933054396 performance\n",
      "[0.9755215050028858, 0.855803342392611, 0.011714084185001972, 0.3599780644783639] x_sample\n",
      "0.399581589958159 performance\n",
      "[0.729990562424058, 0.17162967726144052, 0.5210366062041293, 0.05433798833925363] x_sample\n",
      "0.7280334728033473 performance\n",
      "[0.19999652489640007, 0.01852179446061397, 0.7936977033574206, 0.22392468806038013] x_sample\n",
      "0.6464435146443515 performance\n",
      "[0.3453516806969027, 0.9280812934655909, 0.7044144019235328, 0.03183892953130785] x_sample\n",
      "0.3619246861924686 performance\n",
      "[0.16469415649791275, 0.6214784014997635, 0.5772285886041676, 0.23789282137450862] x_sample\n",
      "0.25313807531380755 performance\n",
      "[0.9342139979247938, 0.613965955965896, 0.5356328030249583, 0.589909976354571] x_sample\n",
      "0.399581589958159 performance\n",
      "[0.7301220295167696, 0.31194499547960186, 0.3982210622160919, 0.20984374897512215] x_sample\n",
      "0.6464435146443515 performance\n",
      "[0.18619300588033616, 0.9443723899839336, 0.7395507950492876, 0.4904588086175671] x_sample\n",
      "0.32217573221757323 performance\n",
      "[0.22741462797332324, 0.25435648177039294, 0.05802916032387562, 0.4344166255581208] x_sample\n",
      "0.5983263598326359 performance\n",
      "[0.3117958819941026, 0.6963434888154595, 0.3777518392924809, 0.1796036775596348] x_sample\n",
      "0.2803347280334728 performance\n",
      "[0.02467872839133123, 0.06724963146324858, 0.6793927734985673, 0.4536968445560453] x_sample\n",
      "0.5627615062761506 performance\n",
      "[0.5365792111087222, 0.8966712930403421, 0.9903389473967044, 0.21689698439847394] x_sample\n",
      "0.399581589958159 performance\n",
      "[0.6630782031001008, 0.26332237673715064, 0.02065099946572868, 0.7583786538361414] x_sample\n",
      "0.7824267782426778 performance\n",
      "[0.32001715082246784, 0.38346389417189797, 0.5883171135536057, 0.8310484552361904] x_sample\n",
      "0.3577405857740586 performance\n",
      "[0.6289818435911487, 0.8726506554473953, 0.27354203481563577, 0.7980468339125637] x_sample\n",
      "0.399581589958159 performance\n",
      "[0.1856359443059522, 0.9527916569719446, 0.6874882763878153, 0.21550767711355845] x_sample\n",
      "0.2803347280334728 performance\n",
      "[0.9473705904889242, 0.7308558067701578, 0.25394164259502583, 0.21331197736748198] x_sample\n",
      "0.399581589958159 performance\n",
      "[0.5182007139306632, 0.025662718054531575, 0.2074700754411094, 0.42468546875150626] x_sample\n",
      "0.7928870292887029 performance\n",
      "[0.37416998033422555, 0.4635754243648107, 0.2776287062947319, 0.5867843464581688] x_sample\n",
      "0.3305439330543933 performance\n",
      "[0.8638556059232314, 0.11753185596203308, 0.5173791071541142, 0.1320681063451533] x_sample\n",
      "0.7573221757322176 performance\n",
      "[0.7168596811925937, 0.39605970280729375, 0.565421311858509, 0.18327983621407862] x_sample\n",
      "0.39330543933054396 performance\n",
      "[0.14484775934337724, 0.48805628064895457, 0.3556127378499556, 0.940431945252813] x_sample\n",
      "0.2615062761506276 performance\n",
      "[0.7653252538069653, 0.7486636198505473, 0.9037197397459334, 0.08342243544201855] x_sample\n",
      "0.399581589958159 performance\n",
      "[0.5521924699224066, 0.5844760689557689, 0.961936378547229, 0.29214752679254885] x_sample\n",
      "0.399581589958159 performance\n",
      "[0.24082877991544682, 0.10029394226549782, 0.016429629591474204, 0.9295293167921905] x_sample\n",
      "0.6213389121338913 performance\n",
      "[0.66991654659091, 0.7851529120231378, 0.2817301057539491, 0.5864101661863267] x_sample\n",
      "0.399581589958159 performance\n",
      "[0.06395526612098112, 0.4856275959346229, 0.9774951397444468, 0.8765052453165908] x_sample\n",
      "0.3200836820083682 performance\n",
      "[0.33815895183684563, 0.9615701545414985, 0.2317016264712045, 0.9493188224156814] x_sample\n",
      "0.3179916317991632 performance\n",
      "[0.9413777047064986, 0.7992025873523917, 0.6304479368667911, 0.874287966624947] x_sample\n",
      "0.399581589958159 performance\n",
      "[0.2930202845077967, 0.8489435553129182, 0.6178766919175238, 0.01323685775889949] x_sample\n",
      "0.3179916317991632 performance\n",
      "[0.34723351793221957, 0.14814086094816503, 0.9818293898182532, 0.47837030703998806] x_sample\n",
      "0.5502092050209205 performance\n",
      "[0.4973913654986627, 0.6394725163987236, 0.3685846061296175, 0.13690027168559893] x_sample\n",
      "0.399581589958159 performance\n",
      "[0.8221177331942455, 0.18984791190275796, 0.511318982546456, 0.22431702897473926] x_sample\n",
      "0.7154811715481172 performance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09784448449403405, 0.8621915174216833, 0.9729194890231303, 0.9608346580630002] x_sample\n",
      "0.3284518828451883 performance\n",
      "[0.906555499221179, 0.7740473326986388, 0.3331451520286419, 0.08110138998799676] x_sample\n",
      "0.399581589958159 performance\n",
      "[0.40724117141380733, 0.23223414217094274, 0.13248763475798297, 0.05342718178682526] x_sample\n",
      "0.6276150627615062 performance\n",
      "[0.7255943642105788, 0.011427458625031028, 0.7705807485027762, 0.14694664540037505] x_sample\n",
      "0.7301255230125523 performance\n",
      "[0.07952208258675575, 0.08960303423860538, 0.6720478073539145, 0.24536720985284477] x_sample\n",
      "0.5669456066945606 performance\n",
      "[0.42053946668009845, 0.5573687913239169, 0.8605511738287938, 0.7270442627113283] x_sample\n",
      "0.38493723849372385 performance\n",
      "[0.27032790523871464, 0.1314827992911276, 0.05537432042119794, 0.3015986344809425] x_sample\n",
      "0.600418410041841 performance\n",
      "[0.26211814923967824, 0.45614056680047965, 0.6832813355476804, 0.6956254456388572] x_sample\n",
      "0.3472803347280335 performance\n",
      "[0.28351884658216664, 0.3799269559001205, 0.18115096173690304, 0.7885455123065187] x_sample\n",
      "0.5397489539748954 performance\n",
      "[0.0568480764332403, 0.6969972417249873, 0.7786953959411034, 0.7774075618487531] x_sample\n",
      "0.28451882845188287 performance\n",
      "[0.25942256434535493, 0.37381313793256143, 0.587599635196389, 0.272821902424467] x_sample\n",
      "0.3263598326359833 performance\n",
      "[0.3708527992178887, 0.19705428018563964, 0.4598558837560074, 0.044612301254114084] x_sample\n",
      "0.6924686192468619 performance\n",
      "[0.799795884570618, 0.07695644698663273, 0.518835148831526, 0.3068100995451961] x_sample\n",
      "0.7698744769874477 performance\n",
      "[0.5775429488313755, 0.9594333408334251, 0.6455702444560039, 0.03536243575549092] x_sample\n",
      "0.399581589958159 performance\n",
      "[0.43040243950806123, 0.5100168523182502, 0.536177494703452, 0.6813925106038379] x_sample\n",
      "0.38493723849372385 performance\n",
      "[0.2775960977317661, 0.1288605654663202, 0.39267567654709434, 0.9564057227959488] x_sample\n",
      "0.6924686192468619 performance\n",
      "[0.18713089175084474, 0.903983954928237, 0.5438059500773263, 0.4569114216457658] x_sample\n",
      "0.27615062761506276 performance\n",
      "[0.8820414102298896, 0.45860396176858587, 0.7241676366115433, 0.399025321703102] x_sample\n",
      "0.38284518828451886 performance\n",
      "[0.9040443929009577, 0.6900250201912274, 0.6996220542505167, 0.3277204015571189] x_sample\n",
      "0.399581589958159 performance\n",
      "[0.7567786427368892, 0.6360610554471413, 0.24002027337970955, 0.16053882248525642] x_sample\n",
      "0.399581589958159 performance\n",
      "[0.7963914745173317, 0.9591666030352225, 0.45813882726004285, 0.5909841653236849] x_sample\n",
      "0.399581589958159 performance\n",
      "[0.8577226441935546, 0.45722345335385706, 0.9518744768327362, 0.5757511620448724] x_sample\n",
      "0.3912133891213389 performance\n",
      "[0.820767120701315, 0.9088437184127384, 0.8155238187685688, 0.15941446344895593] x_sample\n",
      "0.399581589958159 performance\n",
      "[0.6288984390617004, 0.3984342586196771, 0.0627129520233457, 0.42403225188984195] x_sample\n",
      "0.7740585774058577 performance\n",
      "[0.2586840668894077, 0.8490383084285108, 0.03330462654669619, 0.9589827218634736] x_sample\n",
      "0.2405857740585774 performance\n",
      "[0.3553688484719296, 0.3567068904025429, 0.01632850268370789, 0.18523232523618394] x_sample\n",
      "0.600418410041841 performance\n",
      "Best Hyperparameters: [0.9883738380592262, 0.10204481074802807, 0.2088767560948347, 0.16130951788499626]\n",
      "Best Performance (Accuracy): 0.7928870292887029\n",
      "pred_y [0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0\n",
      " 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1\n",
      " 1 1 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0\n",
      " 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 1 0\n",
      " 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1\n",
      " 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1\n",
      " 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0\n",
      " 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 0 1 0 1\n",
      " 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 1\n",
      " 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 1\n",
      " 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1\n",
      " 1 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0\n",
      " 0 1 0 1 1 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load your dataset here (replace this with your actual dataset)\n",
    "# For demonstration, let's create a random dataset\n",
    "np.random.seed(0)\n",
    "X = np.array(X_train) #np.random.rand(428, 4)  # Predictor variables\n",
    "y = np.array(y_train) #np.random.randint(2, size=428)  # Binary target variable (0 or 1)\n",
    "print(X)\n",
    "# Objective function (e.g., accuracy of a model)\n",
    "def objective_function(x):\n",
    "    coefficients = x \n",
    "    y_pred = np.dot(X, coefficients) \n",
    "    y_pred_binary = (y_pred >= 0.5).astype(int) \n",
    "    accuracy = np.mean(y_pred_binary == y)  # Calculate accuracy\n",
    "    return accuracy, y_pred_binary\n",
    "\n",
    "# Define search space (x_range) for each predictor variable\n",
    "# You would replace these ranges with your actual ranges based on your data and domain knowledge\n",
    "x1_range = [0, 1]  # Example range for predictor variable 1\n",
    "x2_range = [0, 1]  # Example range for predictor variable 2\n",
    "x3_range = [0, 1]  # Example range for predictor variable 3\n",
    "x4_range = [0, 1]  # Example range for predictor variable 4\n",
    "\n",
    "# Combine ranges into a single search space\n",
    "x_range = [x1_range, x2_range, x3_range, x4_range]\n",
    "\n",
    "# Bayesian optimization\n",
    "def bayesian_optimization(objective_function_0, x_range, num_iterations):\n",
    "    best_hyperparameters = None\n",
    "    best_performance = -float('inf')\n",
    "    \n",
    "    for _ in range(num_iterations):\n",
    "        # Randomly sample hyperparameters from the search space\n",
    "        x_sample = [np.random.uniform(low=x_range[i][0], high=x_range[i][1]) for i in range(len(x_range))]\n",
    "        print(x_sample, 'x_sample')\n",
    "        # Evaluate objective function with the sampled hyperparameters\n",
    "        performance = objective_function_0(x_sample)[0]\n",
    "        predicted_values = objective_function_0(x_sample)[1]\n",
    "        print(performance, 'performance')\n",
    "        # Update best hyperparameters if performance improves\n",
    "        if performance > best_performance:\n",
    "            best_hyperparameters = x_sample\n",
    "            best_performance = performance\n",
    "            predicted_vals = predicted_values\n",
    "    \n",
    "    return best_hyperparameters, best_performance, predicted_vals\n",
    "\n",
    "# Example usage\n",
    "num_iterations = 100\n",
    "\n",
    "best_hyperparameters, best_performance, pred_y =  bayesian_optimization(objective_function, x_range, num_iterations)\n",
    "print(\"Best Hyperparameters:\", best_hyperparameters)\n",
    "print(\"Best Performance (Accuracy):\", best_performance)\n",
    "print('pred_y', pred_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "83dbad0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(y_train, pred_y)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "feaa514d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379\n",
      "478\n",
      "0.7928870292887029\n"
     ]
    }
   ],
   "source": [
    "print(calculate_accuracy(y_train,pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "de17861c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: [0.006272517160175406, 0.7129796823766446, 0.31738726323789, -0.6741311458371406, -0.8588625051991403, 0.2848385564126312, -0.9469773789167564, 0.1715511625469266, 0.8804604828499152, 0.1509483557517579, -0.22366014758695618, 0.28657643688470635, -0.08349421901696674, 0.09123357863186987, 0.8829296175530503, -0.2277947243984515, 0.9223811276478284, 0.8107012839121275, -0.6084177304214071, -0.8612773982496691, -0.7984439972451467, -0.9635563486969005, -0.8111140784881432, 0.3660135468327137, -0.857622703079542]\n",
      "Best loss: 0.4138790781738342\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Any, Dict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load your dataset here (replace this with your actual dataset)\n",
    "df = pd.read_csv('train.csv').dropna(subset=['Age', 'Fare', 'Sex'])\n",
    "target_var = df.Survived\n",
    "df = df[['Sex', 'Age', 'Fare']]\n",
    "encode_cat_var = pd.get_dummies(df['Sex'])\n",
    "df = df.drop(['Sex'], axis=1)\n",
    "X = pd.concat([encode_cat_var, df], axis=1)\n",
    "scaler = MinMaxScaler()\n",
    "X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "y = target_var.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Activation functions\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Loss function\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    cost = np.mean(np.abs(y_true - y_pred))  # Example: Mean absolute error\n",
    "    return cost\n",
    "\n",
    "# Feed-forward ANN\n",
    "def forward_prop(W1, b1, W2, b2, X_data):\n",
    "    Z1 = np.dot(X_data, W1) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    return A2\n",
    "\n",
    "# Evaluation function\n",
    "def black_box_function(W1, b1, W2, b2):\n",
    "    result = forward_prop(W1, b1, W2, b2, X_train)\n",
    "    return binary_cross_entropy(y_train, result)\n",
    "\n",
    "# Define search space parameters\n",
    "def get_search_space_params() -> List[Dict[str, Any]]:\n",
    "    params = [{'name': f'W1_{i}{j}', 'type': 'num', 'lb': -1, 'ub': 1} for i in range(4) for j in range(4)]\n",
    "    params.extend([{'name': f'b1_{i}', 'type': 'num', 'lb': -1, 'ub': 1} for i in range(4)])\n",
    "    params.extend([{'name': f'W2_{i}{j}', 'type': 'num', 'lb': -1, 'ub': 1} for i in range(1) for j in range(4)])\n",
    "    params.extend([{'name': f'b2_{i}', 'type': 'num', 'lb': -1, 'ub': 1} for i in range(1)])\n",
    "    return params\n",
    "\n",
    "# Creating task and optimization\n",
    "def optimize(epochs):\n",
    "    np.random.seed(42)\n",
    "    search_space_params = get_search_space_params()\n",
    "    best_loss = float('inf')\n",
    "    best_params = None\n",
    "    for _ in range(epochs):\n",
    "        params = [np.random.uniform(p['lb'], p['ub']) for p in search_space_params]\n",
    "        W1 = np.array(params[:16]).reshape(4, 4)\n",
    "        b1 = np.array(params[16:20]).reshape(1, 4)\n",
    "        W2 = np.array(params[20:24]).reshape(4, 1)\n",
    "        b2 = np.array(params[24]).reshape(1, 1)\n",
    "        loss = black_box_function(W1, b1, W2, b2)\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_params = params\n",
    "    return best_params, best_loss\n",
    "\n",
    "# Example usage\n",
    "epochs = 20\n",
    "best_params, best_loss = optimize(epochs)\n",
    "print('Best parameters:', best_params)\n",
    "print('Best loss:', best_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e1188fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 12)\n",
      "(714, 12)\n",
      "['W0' 'b0' 'W1' 'b1' 'W2' 'b2']\n",
      "Evaluating with parameters:\n",
      "xind shape (47,)\n",
      "[array(['W0', 'b0', 'W1', 'b1', 'W2', 'b2'], dtype='<U2')] arr args\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 6 into shape (4,5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 173\u001b[0m\n\u001b[0;32m    170\u001b[0m x_next_reshaped \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(x_next)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Reshape x_next to have shape (1, 25)\u001b[39;00m\n\u001b[0;32m    171\u001b[0m x_next_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(x_next_reshaped, columns\u001b[38;5;241m=\u001b[39mtask\u001b[38;5;241m.\u001b[39mget_parameter_names())\n\u001b[1;32m--> 173\u001b[0m y_next \u001b[38;5;241m=\u001b[39m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_next_df\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    174\u001b[0m weights_list\u001b[38;5;241m.\u001b[39mappend(task\u001b[38;5;241m.\u001b[39mevaluate(x_next_df)[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    177\u001b[0m opt\u001b[38;5;241m.\u001b[39mobserve(x_next, y_next)\n",
      "Cell \u001b[1;32mIn[1], line 129\u001b[0m, in \u001b[0;36mCustomTask.evaluate\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;66;03m#print(\"x_ind:\", x_ind)\u001b[39;00m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxind shape\u001b[39m\u001b[38;5;124m'\u001b[39m , x_ind\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m--> 129\u001b[0m     y[ind] \u001b[38;5;241m=\u001b[39m \u001b[43mblack_box_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_sizes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m  x, y\n",
      "Cell \u001b[1;32mIn[1], line 105\u001b[0m, in \u001b[0;36mblack_box_function\u001b[1;34m(layer_sizes, *args)\u001b[0m\n\u001b[0;32m    103\u001b[0m arr_args \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39marray(arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]  \u001b[38;5;66;03m# Convert each argument to a NumPy array\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28mprint\u001b[39m(arr_args, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marr args\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 105\u001b[0m reshaped_args \u001b[38;5;241m=\u001b[39m [arg\u001b[38;5;241m.\u001b[39mreshape(size) \u001b[38;5;28;01mfor\u001b[39;00m arg, size \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(arr_args, size_tuples)]\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# Now `reshaped_args` contains NumPy arrays of the same shape, suitable for further processing\u001b[39;00m\n\u001b[0;32m    107\u001b[0m result \u001b[38;5;241m=\u001b[39m FeedForwardNN(reshaped_args, X_train)\n",
      "Cell \u001b[1;32mIn[1], line 105\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    103\u001b[0m arr_args \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39marray(arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]  \u001b[38;5;66;03m# Convert each argument to a NumPy array\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28mprint\u001b[39m(arr_args, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marr args\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 105\u001b[0m reshaped_args \u001b[38;5;241m=\u001b[39m [\u001b[43marg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m arg, size \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(arr_args, size_tuples)]\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# Now `reshaped_args` contains NumPy arrays of the same shape, suitable for further processing\u001b[39;00m\n\u001b[0;32m    107\u001b[0m result \u001b[38;5;241m=\u001b[39m FeedForwardNN(reshaped_args, X_train)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 6 into shape (4,5)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import List, Tuple, Any, Dict\n",
    "from mcbo.optimizers.bo_builder import BoBuilder\n",
    "from mcbo.tasks.task_base import TaskBase\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "print(df.shape)\n",
    "df = df.dropna(subset=['Age', 'Fare', 'Sex'])\n",
    "target_var = df.Survived\n",
    "print(df.shape)\n",
    "\n",
    "df = df[['Sex','Age', 'Fare']]\n",
    "encode_cat_var = pd.get_dummies(df['Sex'])\n",
    "df = df.drop(['Sex'], axis =1)\n",
    "X = pd.concat([encode_cat_var, df], axis = 1)\n",
    "X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns) \n",
    "\n",
    "y = target_var\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "def label_parameters(layer_sizes):\n",
    "    labels = {}\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        labels[f\"W{i}\"] = f\"{i}\"\n",
    "        labels[f\"b{i}\"] = f\"{i}\"\n",
    "    return labels\n",
    "\n",
    "class FeedForwardNN:\n",
    "    def __init__(self, parameter_labels, X_data):\n",
    "        self.parameter_labels = parameter_labels\n",
    "        self.parameters = {}\n",
    "        for param, label in self.parameter_labels:             #.items():\n",
    "            self.parameters[param] = None  # Initialize parameters to None\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        for param in self.parameter_labels.keys():\n",
    "            # Initialize parameters randomly or with zeros, depending on your preference\n",
    "            self.parameters[param] = np.random.randn(3, 3)  # Example initialization\n",
    "\n",
    "    def forward_propagation(self, X_data):\n",
    "        # Implement forward propagation using parameters stored in self.parameters\n",
    "        pass  # Placeholder for implementation\n",
    "\n",
    "  #  def train(self, X_train, y_train):\n",
    "        # Placeholder for training process\n",
    "   #     pass\n",
    "\n",
    "# Example usage:\n",
    "layer_sizes = np.array([4, 5, 3, 1])  # Example list of layer sizes\n",
    "parameter_labels = label_parameters(layer_sizes)  # Using previously defined label_parameters function\n",
    "#nn = FeedForwardNN(parameter_labels)\n",
    "#nn.initialize_parameters()  # Initialize parameters\n",
    "#activations = nn.forward_propagation(X_train)\n",
    "parameters =np.array(list( parameter_labels.keys()))\n",
    "print((parameters))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Activation functions\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def softmax(Z):\n",
    "    return np.exp(Z) / np.sum(np.exp(Z))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Loss function\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    '''epsilon = 1e-15  # Small value to avoid numerical instability\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip predicted probabilities to avoid taking the log of zero or one\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "'''\n",
    "    cost = np.mean(np.abs(y_true - y_pred))  # Example: Mean absolute error\n",
    "    return cost\n",
    "\n",
    "def generate_sizes(layer_sizes):\n",
    "    sizes = []\n",
    "    start_idx = 0\n",
    "    for size in layer_sizes:\n",
    "        end_idx = start_idx + size\n",
    "        sizes.append(end_idx - start_idx)\n",
    "        start_idx = end_idx\n",
    "    return sizes\n",
    "\n",
    "\n",
    "def black_box_function(*args, layer_sizes):\n",
    "    size_tuples = [(layer_sizes[i], layer_sizes[i+1]) for i in range(len(layer_sizes)-1)]\n",
    "    arr_args = [np.array(arg) for arg in args]  # Convert each argument to a NumPy array\n",
    "    print(arr_args, 'arr args')\n",
    "    reshaped_args = [arg.reshape(size) for arg, size in zip(arr_args, size_tuples)]\n",
    "    # Now `reshaped_args` contains NumPy arrays of the same shape, suitable for further processing\n",
    "    result = FeedForwardNN(reshaped_args, X_train)\n",
    "    y_train_encoded = np.array([y_train])  # np.eye(2)[y_train]\n",
    "    return binary_cross_entropy(y_train_encoded, result)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class CustomTask(TaskBase):\n",
    "    def __init__(self, pars, layer_sizes):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.pars = pars \n",
    "\n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return 'Custom Task'\n",
    "    \n",
    "    def evaluate(self, x: np.ndarray) -> Tuple[pd.DataFrame ,np.ndarray]:\n",
    "        y = np.zeros((len(x), 1))\n",
    "        for ind in range(len(x)):\n",
    "            x_ind = x.iloc[ind].to_numpy()  # Convert Series to NumPy array\n",
    "            print(\"Evaluating with parameters:\")\n",
    "            #print(\"x_ind:\", x_ind)\n",
    "            print('xind shape' , x_ind.shape)\n",
    "            y[ind] = black_box_function(self.pars, layer_sizes = self.layer_sizes)\n",
    "        return  x, y\n",
    "\n",
    "    \n",
    "    def get_search_space_params(self) -> List[Dict[str, Any]]:\n",
    "        params = []\n",
    "        start_idx = 0\n",
    "        for layer, size in enumerate(self.layer_sizes[:-1]):\n",
    "            for i in range(size):\n",
    "                for j in range(self.layer_sizes[layer + 1]):\n",
    "                    params.append({'name': f'W{layer}_{i}{j}', 'type': 'num', 'lb': -1, 'ub': 1})\n",
    "            for i in range(self.layer_sizes[layer + 1]):\n",
    "                params.append({'name': f'b{layer}_{i}', 'type': 'num', 'lb': -1, 'ub': 1})\n",
    "        return params\n",
    "        \n",
    "\n",
    "    def get_parameter_names(self) -> List[str]:\n",
    "        params = []\n",
    "        for layer, size in enumerate(self.layer_sizes[:-1]):\n",
    "            for i in range(size):\n",
    "                for j in range(self.layer_sizes[layer + 1]):\n",
    "                    params.append(f'W{layer}_{i}{j}')\n",
    "            for i in range(self.layer_sizes[layer + 1]):\n",
    "                params.append(f'b{layer}_{i}')\n",
    "        return params\n",
    "\n",
    "# Creating task and optimization\n",
    "task = CustomTask(parameters, layer_sizes)\n",
    "searchspace = task.get_search_space()\n",
    "\n",
    "epochs = 100\n",
    "bo_builder = BoBuilder(model_id='gp_rd', acq_opt_id='is', acq_func_id='ei', tr_id='basic')\n",
    "opt = bo_builder.build_bo(search_space=task.get_search_space(), n_init=200)\n",
    "\n",
    "\n",
    "# Optimization loop\n",
    "budget_eval = epochs\n",
    "# Inside the optimization loop\n",
    "weights_list = []\n",
    "for _ in range(budget_eval):\n",
    "    x_next = opt.suggest()\n",
    "    x_next_reshaped = np.array(x_next).reshape(1, -1)  # Reshape x_next to have shape (1, 25)\n",
    "    x_next_df = pd.DataFrame(x_next_reshaped, columns=task.get_parameter_names())\n",
    "    \n",
    "    y_next = task.evaluate(x_next_df)[1]\n",
    "    weights_list.append(task.evaluate(x_next_df)[0])\n",
    "    \n",
    "   \n",
    "    opt.observe(x_next, y_next)\n",
    "   \n",
    "\n",
    "    \n",
    "# Printing best result\n",
    "print('Best parameters:', opt.best_x)\n",
    "print('Best loss:', opt.best_y)\n",
    "\n",
    "# Plotting version\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "y = opt.data_buffer.y.numpy()\n",
    "regret_y = np.min(np.cumsum(y, axis=1), axis=0)\n",
    "#plt.plot(np.arange(1,  budget_eval ), regret_y)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "32db302a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (83521, 4)\n",
      "Shape of y_binary: (83521,)\n",
      "opt ======== <mcbo.optimizers.bo_base.BoBase object at 0x0000020811A953A0>\n",
      "{'W0': array([[ 0.75386303, -0.2496896 ,  0.73221999, -0.96464214],\n",
      "       [-0.90890688,  0.70570393, -0.51083313, -0.65875962],\n",
      "       [ 0.46190996,  0.56037828, -0.97837064,  0.94476942],\n",
      "       [-0.89333488, -0.24443206, -0.48435411, -0.21433241],\n",
      "       [-0.42928374,  0.22868709,  0.26456907, -0.25346136]]), 'b0': array([[ 0.81484518],\n",
      "       [-0.23393458],\n",
      "       [ 0.53068482],\n",
      "       [-0.72632915],\n",
      "       [-0.55182547]]), 'W1': array([[ 0.30010147, -0.63604584, -0.06168803, -0.71130367, -0.22645877],\n",
      "       [ 0.62220536,  0.71628871, -0.07101131,  0.45538753, -0.58250687],\n",
      "       [ 0.23595169,  0.27362234, -0.02473506,  0.27283095,  0.69114687]]), 'b1': array([[-0.5069339 ],\n",
      "       [ 0.84502607],\n",
      "       [-0.85363567]]), 'W2': array([[-0.68824404, -0.91556399, -0.70506563]]), 'b2': array([[0.29255867]])}\n",
      "{'W0': array([[-0.79522228,  0.57259586, -0.04089277, -0.16215895],\n",
      "       [ 0.01643156,  0.65129047,  0.67527842, -0.37640387],\n",
      "       [-0.72580086,  0.47555544,  0.81847777,  0.89864995],\n",
      "       [ 0.29383132, -0.87201837, -0.4280464 , -0.53481968],\n",
      "       [ 0.2255345 ,  0.91624123, -0.22226936, -0.27122379]]), 'b0': array([[-0.92864967],\n",
      "       [ 0.3873643 ],\n",
      "       [-0.16889292],\n",
      "       [ 0.84844478],\n",
      "       [-0.29626076]]), 'W1': array([[-0.04310812,  0.29406552, -0.767929  , -0.42021174, -0.69210243],\n",
      "       [-0.83405123,  0.06285362, -0.49280677, -0.31964358,  0.36672309],\n",
      "       [-0.26693427, -0.86131728,  0.83317934,  0.34309887, -0.18101179]]), 'b1': array([[ 0.02748275],\n",
      "       [-0.5033543 ],\n",
      "       [-0.86538948]]), 'W2': array([[ 0.31128476, -0.41351499, -0.93607922]]), 'b2': array([[0.01492281]])}\n",
      "{'W0': array([[-0.14395986, -0.35343012, -0.40329946, -0.66245022],\n",
      "       [ 0.78260021,  0.05365605, -0.38209551, -0.63116937],\n",
      "       [-0.34630728, -0.82372764, -0.47647788, -0.58159833],\n",
      "       [-0.90007802, -0.14975702,  0.56657621, -0.05214381],\n",
      "       [-0.5081673 , -0.4311519 ,  0.22624237,  0.49163979]]), 'b0': array([[ 0.53781421],\n",
      "       [ 0.74233649],\n",
      "       [-0.034649  ],\n",
      "       [ 0.4449676 ],\n",
      "       [-0.09257761]]), 'W1': array([[-0.74707987,  0.10878995, -0.70607824,  0.90579416, -0.78004285],\n",
      "       [-0.67663788,  0.21136863, -0.36969128, -0.49546234, -0.55041103],\n",
      "       [ 0.56887939, -0.88480533,  0.94849931,  0.29240907,  0.31765936]]), 'b1': array([[-0.00678729],\n",
      "       [ 0.71431002],\n",
      "       [-0.4177934 ]]), 'W2': array([[-0.29879786, -0.65926975,  0.65025417]]), 'b2': array([[-0.13548356]])}\n",
      "{'W0': array([[ 0.93705504,  0.11075   ,  0.02117772, -0.83463953],\n",
      "       [ 0.55724206, -0.89285691,  0.89090604,  0.10145714],\n",
      "       [ 0.0309497 ,  0.40704211, -0.66070962,  0.52227858],\n",
      "       [ 0.57911061,  0.36298434, -0.20089653,  0.80146489],\n",
      "       [-0.52360859,  0.25023494, -0.01667803,  0.6210149 ]]), 'b0': array([[-0.77877315],\n",
      "       [-0.94881826],\n",
      "       [-0.13641052],\n",
      "       [ 0.76947672],\n",
      "       [-0.93488893]]), 'W1': array([[-0.41996113, -0.59018087, -0.536174  , -0.13758211,  0.71732814],\n",
      "       [ 0.3039691 ,  0.89238625,  0.4634672 , -0.61793027,  0.45689453],\n",
      "       [ 0.45554512,  0.80404263, -0.82019896,  0.17554174,  0.0747773 ]]), 'b1': array([[ 0.84819976],\n",
      "       [-0.04351726],\n",
      "       [-0.18504244]]), 'W2': array([[ 0.83657592,  0.69630935, -0.35583453]]), 'b2': array([[-0.0878155]])}\n",
      "{'W0': array([[-0.08019072, -0.16920944, -0.08574255,  0.89047692],\n",
      "       [-0.32694459, -0.39439035, -0.07792264, -0.23464261],\n",
      "       [-0.4172301 ,  0.31441269,  0.47382265, -0.77255202],\n",
      "       [-0.87972264,  0.05523842, -0.3093679 ,  0.23792045],\n",
      "       [-0.09494545, -0.43035256,  0.02880539,  0.31123061]]), 'b0': array([[-0.50429791],\n",
      "       [-0.10601951],\n",
      "       [ 0.6702684 ],\n",
      "       [-0.53423065],\n",
      "       [-0.40170829]]), 'W1': array([[-0.7081536 ,  0.77776416, -0.89644078, -0.37238385,  0.96532722],\n",
      "       [-0.218772  , -0.10845967, -0.47092161,  0.18019509, -0.30826545],\n",
      "       [-0.07737667, -0.70064478, -0.13845742,  0.0642205 , -0.98708311]]), 'b1': array([[ 0.48242699],\n",
      "       [-0.17949633],\n",
      "       [-0.35753847]]), 'W2': array([[0.72873375, 0.13479463, 0.84509339]]), 'b2': array([[0.31197166]])}\n",
      "{'W0': array([[ 0.49061639,  0.07653638, -0.86667665, -0.56863632],\n",
      "       [ 0.10541539, -0.06002599,  0.87972423,  0.40964307],\n",
      "       [ 0.243791  ,  0.20944627,  0.63967679, -0.97007661],\n",
      "       [-0.47825664, -0.84866787,  0.93575006,  0.20736816],\n",
      "       [-0.55211286, -0.70571957,  0.14917553,  0.29681918]]), 'b0': array([[ 0.69564872],\n",
      "       [-0.59174534],\n",
      "       [-0.02647968],\n",
      "       [-0.78611744],\n",
      "       [ 0.73663857]]), 'W1': array([[-0.65117391,  0.3298899 ,  0.03229195, -0.52635302, -0.75655727],\n",
      "       [ 0.54639469, -0.43981258, -0.249413  , -0.59684823, -0.26639198],\n",
      "       [ 0.06396486, -0.66651515, -0.48866149, -0.36032606,  0.26267606]]), 'b1': array([[0.20169618],\n",
      "       [0.62397752],\n",
      "       [0.67523728]]), 'W2': array([[-0.55936396, -0.24266782, -0.22097651]]), 'b2': array([[0.10436218]])}\n",
      "{'W0': array([[-0.54581717, -0.56008242,  0.2313598 ,  0.90017249],\n",
      "       [-0.75059887, -0.64855325, -0.39674285, -0.41258809],\n",
      "       [-0.00689535, -0.42296922, -0.42949501, -0.52098946],\n",
      "       [ 0.20389613,  0.989883  ,  0.12760544, -0.51302318],\n",
      "       [-0.54008279,  0.97035612,  0.22145196,  0.98359893]]), 'b0': array([[ 0.07001005],\n",
      "       [-0.93852995],\n",
      "       [-0.10462894],\n",
      "       [ 0.45563243],\n",
      "       [ 0.51942968]]), 'W1': array([[-0.39719847,  0.18376904,  0.89467868,  0.99056915,  0.73056113],\n",
      "       [-0.04340176, -0.43323135, -0.69318907,  0.08948373,  0.40786868],\n",
      "       [ 0.87887134, -0.36024862,  0.65608853, -0.3743945 ,  0.6549584 ]]), 'b1': array([[0.54418013],\n",
      "       [0.75976474],\n",
      "       [0.6590277 ]]), 'W2': array([[-0.92289345,  0.57468079,  0.23812404]]), 'b2': array([[0.67174111]])}\n",
      "{'W0': array([[-0.63612385, -0.10666762,  0.85782811, -0.01542505],\n",
      "       [ 0.74730473, -0.68625622,  0.76456694,  0.58253063],\n",
      "       [ 0.53682027, -0.8463369 ,  0.58590881,  0.76102553],\n",
      "       [-0.43741442, -0.8013    , -0.33772148,  0.32173445],\n",
      "       [-0.20961456,  0.51448942,  0.1243636 , -0.12916925]]), 'b0': array([[-0.05546654],\n",
      "       [-0.44182624],\n",
      "       [ 0.61304684],\n",
      "       [-0.07220955],\n",
      "       [-0.86807244]]), 'W1': array([[ 0.25249414,  0.59107284, -0.75586451,  0.26044424,  0.9267794 ],\n",
      "       [-0.36182191,  0.51606844,  0.41944264,  0.54814311,  0.15191743],\n",
      "       [ 0.94996948,  0.92061815, -0.79045641, -0.21506092, -0.96143333]]), 'b1': array([[0.50328497],\n",
      "       [0.59000705],\n",
      "       [0.54056803]]), 'W2': array([[ 0.05261268, -0.78321304, -0.86577801]]), 'b2': array([[-0.96691014]])}\n",
      "{'W0': array([[-0.75235323, -0.14321391, -0.38886447,  0.99315059],\n",
      "       [ 0.1842716 ,  0.23651862,  0.71738614, -0.58453403],\n",
      "       [-0.34107434,  0.28103721, -0.54321918, -0.2992884 ],\n",
      "       [-0.22239451,  0.48260899, -0.66273086,  0.80354429],\n",
      "       [-0.57063566,  0.76303671,  0.23763021, -0.45737666]]), 'b0': array([[ 0.6128915 ],\n",
      "       [ 0.02226634],\n",
      "       [-0.85826812],\n",
      "       [ 0.64870019],\n",
      "       [-0.80536097]]), 'W1': array([[ 0.730849  ,  0.03968553,  0.81050353,  0.80801555, -0.41598392],\n",
      "       [ 0.10255642,  0.60851376, -0.57394489,  0.39131398, -0.83688509],\n",
      "       [-0.21506571,  0.11072207,  0.87011301, -0.81026729,  0.42650437]]), 'b1': array([[ 0.84683288],\n",
      "       [ 0.73143636],\n",
      "       [-0.73862285]]), 'W2': array([[ 0.93829966,  0.6079064 , -0.90655313]]), 'b2': array([[0.3925599]])}\n",
      "{'W0': array([[-0.00343713, -0.15493026, -0.27943978, -0.95162554],\n",
      "       [-0.10760011, -0.06228984,  0.19190072,  0.02775069],\n",
      "       [-0.28335234,  0.84127644, -0.39951339,  0.81653675],\n",
      "       [-0.43010275,  0.20064496, -0.36438521, -0.51501863],\n",
      "       [-0.2947349 ,  0.3762878 ,  0.44689273, -0.63956939]]), 'b0': array([[-0.73943839],\n",
      "       [ 0.11214548],\n",
      "       [ 0.87728887],\n",
      "       [ 0.44127318],\n",
      "       [ 0.16791848]]), 'W1': array([[-0.17476463,  0.22111099, -0.11389467, -0.57745864,  0.54820862],\n",
      "       [-0.12068511, -0.28097463, -0.0281155 ,  0.46777423,  0.26527331],\n",
      "       [-0.08082256,  0.12427402,  0.29998435,  0.44524642,  0.50786053]]), 'b1': array([[ 0.07630902],\n",
      "       [ 0.68659667],\n",
      "       [-0.95645357]]), 'W2': array([[ 0.89017404, -0.84963817,  0.41710611]]), 'b2': array([[-0.05297282]])}\n",
      "{'W0': array([[-0.23986044,  0.62297741, -0.89182634,  0.88407312],\n",
      "       [ 0.78868612, -0.72152007, -0.69612079,  0.87245262],\n",
      "       [-0.11846712,  0.72847808,  0.27018781, -0.29746248],\n",
      "       [ 0.45646441, -0.65637653, -0.2223511 , -0.31723298],\n",
      "       [ 0.73501766,  0.19467361, -0.91706813,  0.03141554]]), 'b0': array([[-0.5826581 ],\n",
      "       [ 0.66778553],\n",
      "       [ 0.64919908],\n",
      "       [-0.46323753],\n",
      "       [-0.74990984]]), 'W1': array([[-0.69316344, -0.08233024, -0.53997179, -0.59759499,  0.84049112],\n",
      "       [-0.17616626, -0.98612973,  0.15259028,  0.3651483 , -0.286725  ],\n",
      "       [ 0.11644653,  0.20354992,  0.60986476,  0.90947   ,  0.52707042]]), 'b1': array([[-0.4218705 ],\n",
      "       [ 0.54650737],\n",
      "       [ 0.4389029 ]]), 'W2': array([[-0.65292163, -0.02716882,  0.4846719 ]]), 'b2': array([[-0.01729138]])}\n",
      "{'W0': array([[ 0.31681048, -0.26304451,  0.9908464 , -0.97733932],\n",
      "       [ 0.3913124 ,  0.16540265, -0.14193408,  0.96282017],\n",
      "       [-0.83454541, -0.71814592, -0.15002745,  0.72894057],\n",
      "       [-0.87830546, -0.09812886, -0.69944105, -0.38025456],\n",
      "       [ 0.17619555,  0.57487173, -0.27225373, -0.70447442]]), 'b0': array([[ 0.96104234],\n",
      "       [-0.49605881],\n",
      "       [-0.85207856],\n",
      "       [-0.49555479],\n",
      "       [-0.36648959]]), 'W1': array([[-0.48704502, -0.32705214,  0.90072503,  0.72344053,  0.9653464 ],\n",
      "       [ 0.76096682,  0.12369754,  0.52736125,  0.04599472, -0.86155455],\n",
      "       [-0.6788267 ,  0.00262357,  0.16971197, -0.2239633 , -0.24975927]]), 'b1': array([[ 0.42189854],\n",
      "       [ 0.32792357],\n",
      "       [-0.9335159 ]]), 'W2': array([[-0.0446212 ,  0.1155253 , -0.65500398]]), 'b2': array([[0.90459791]])}\n",
      "{'W0': array([[ 0.39633845,  0.04250088, -0.37379222,  0.37041599],\n",
      "       [-0.27971194,  0.00175919,  0.64803499, -0.21409374],\n",
      "       [ 0.71376718,  0.70670312, -0.85386543, -0.34050994],\n",
      "       [ 0.26506931, -0.57841047, -0.71388936,  0.34284774],\n",
      "       [-0.72299311,  0.31443006,  0.2974946 ,  0.89398943]]), 'b0': array([[0.56884095],\n",
      "       [0.18322989],\n",
      "       [0.85678384],\n",
      "       [0.80131335],\n",
      "       [0.36869934]]), 'W1': array([[-0.59563627,  0.36925803,  0.0717516 , -0.45465356, -0.19851596],\n",
      "       [-0.00255931,  0.44923922,  0.66809016,  0.54762193,  0.77577065],\n",
      "       [-0.86642003, -0.70790392, -0.81296303,  0.91571799,  0.61710812]]), 'b1': array([[-0.66067696],\n",
      "       [-0.24108781],\n",
      "       [ 0.79627842]]), 'W2': array([[-0.4554149 , -0.2193208 ,  0.41754389]]), 'b2': array([[-0.07258982]])}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W0': array([[ 0.28139746, -0.85224205, -0.45801509, -0.36845002],\n",
      "       [-0.30509529,  0.8588615 , -0.70539895, -0.78156623],\n",
      "       [-0.26850169, -0.21112793, -0.70385756, -0.78374858],\n",
      "       [-0.95826666, -0.56698062,  0.65739341, -0.72203271],\n",
      "       [ 0.31859047,  0.8116534 , -0.49601571,  0.33065221]]), 'b0': array([[-0.3303978 ],\n",
      "       [ 0.30502542],\n",
      "       [ 0.89751557],\n",
      "       [ 0.11834558],\n",
      "       [ 0.49369448]]), 'W1': array([[-0.38131221, -0.10746279, -0.94261587, -0.40702587,  0.91978913],\n",
      "       [-0.51052542, -0.62310619,  0.92412975,  0.48865256,  0.0206053 ],\n",
      "       [ 0.91896739, -0.49150945,  0.08011694, -0.65874742,  0.35158978]]), 'b1': array([[ 0.96238434],\n",
      "       [-0.77173773],\n",
      "       [ 0.02257835]]), 'W2': array([[0.29925691, 0.38002455, 0.78695538]]), 'b2': array([[-0.18744118]])}\n",
      "{'W0': array([[ 0.87618539,  0.17075292, -0.51460454,  0.66637141],\n",
      "       [ 0.68145011,  0.17952311,  0.56134824,  0.84132694],\n",
      "       [-0.87490857, -0.38974116,  0.78442702,  0.82254089],\n",
      "       [ 0.49153667, -0.71288437,  0.11358362, -0.11530186],\n",
      "       [ 0.83212722, -0.8651001 , -0.28007966,  0.69308193]]), 'b0': array([[-0.33891141],\n",
      "       [ 0.32022445],\n",
      "       [-0.68503035],\n",
      "       [ 0.39582869],\n",
      "       [-0.0137791 ]]), 'W1': array([[ 0.14897999, -0.81582732,  0.3834469 ,  0.10170649,  0.43351076],\n",
      "       [-0.31885803,  0.03950981, -0.60919107, -0.45857894,  0.04910356],\n",
      "       [ 0.57821289,  0.59060601, -0.31673633, -0.20480735, -0.38124348]]), 'b1': array([[-0.5164529 ],\n",
      "       [-0.57128546],\n",
      "       [ 0.83439624]]), 'W2': array([[-0.0325237 , -0.63926806, -0.47903519]]), 'b2': array([[-0.23166635]])}\n",
      "{'W0': array([[-0.75439716,  0.58959886,  0.97288906,  0.46175759],\n",
      "       [-0.47279808, -0.76169783, -0.4903033 ,  0.34257961],\n",
      "       [ 0.55034587,  0.54598355,  0.33067342,  0.57523265],\n",
      "       [ 0.83768629,  0.72404537, -0.52498963,  0.75576267],\n",
      "       [-0.73834803,  0.93128596, -0.09175528, -0.86193527]]), 'b0': array([[ 0.86049204],\n",
      "       [ 0.27329567],\n",
      "       [-0.3368912 ],\n",
      "       [-0.93024143],\n",
      "       [ 0.88798646]]), 'W1': array([[-7.35549671e-01,  1.61805058e-01, -1.06337183e-01,\n",
      "        -1.99603954e-04,  5.98506374e-01],\n",
      "       [ 5.01322537e-01,  3.45803501e-01, -2.13973110e-01,\n",
      "        -6.35670066e-01,  1.27751030e-01],\n",
      "       [ 6.07457630e-01,  5.81284880e-01,  3.08210599e-01,\n",
      "        -8.34729588e-01,  3.85643510e-01]]), 'b1': array([[-0.09520325],\n",
      "       [ 0.57595542],\n",
      "       [-0.86090717]]), 'W2': array([[-0.83668709, -0.62368411,  0.15213469]]), 'b2': array([[0.72138931]])}\n",
      "{'W0': array([[-0.16824346,  0.55480329, -0.77138406,  0.75966032],\n",
      "       [ 0.25275674,  0.65934329,  0.85736654, -0.83397816],\n",
      "       [-0.83081301,  0.27876453, -0.83405244,  0.02487528],\n",
      "       [-0.04717962,  0.35339717, -0.42400793,  0.36258333],\n",
      "       [-0.30510094,  0.24508961, -0.01875106,  0.77435438]]), 'b0': array([[-0.37363492],\n",
      "       [-0.81467724],\n",
      "       [-0.6267984 ],\n",
      "       [-0.44489456],\n",
      "       [ 0.32063044]]), 'W1': array([[-0.41595709, -0.2028583 ,  0.2053357 , -0.08791758, -0.44277945],\n",
      "       [-0.17944018,  0.20391329,  0.98160091, -0.76445759,  0.54327493],\n",
      "       [-0.40697657, -0.72671349, -0.35708764,  0.45008167,  0.4681138 ]]), 'b1': array([[ 0.32025081],\n",
      "       [-0.2370002 ],\n",
      "       [-0.78775548]]), 'W2': array([[-0.9456344 ,  0.91379948,  0.7268262 ]]), 'b2': array([[0.3433645]])}\n",
      "{'W0': array([[ 0.05549177,  0.86660708, -0.58753344,  0.77657216],\n",
      "       [-0.2646353 , -0.26855846, -0.35382098, -0.85681464],\n",
      "       [ 0.79073786,  0.49653627, -0.21589045, -0.64882158],\n",
      "       [ 0.18758145, -0.38061365,  0.20189859,  0.39520032],\n",
      "       [ 0.09886371,  0.9867457 ,  0.69085411,  0.91734613]]), 'b0': array([[-0.00201551],\n",
      "       [-0.44262795],\n",
      "       [-0.93591664],\n",
      "       [-0.27916255],\n",
      "       [ 0.3492006 ]]), 'W1': array([[ 5.48131541e-01, -2.14630676e-02,  3.03032326e-01,\n",
      "         1.08737617e-01, -9.74867947e-01],\n",
      "       [ 1.01261398e-01,  6.23923422e-01,  9.84723133e-01,\n",
      "        -9.23861192e-01, -1.39514589e-04],\n",
      "       [-3.54184733e-01,  7.48677333e-01,  2.04852662e-01,\n",
      "        -4.72377019e-01,  6.22522490e-01]]), 'b1': array([[-0.24500544],\n",
      "       [-0.2074397 ],\n",
      "       [-0.75449038]]), 'W2': array([[-0.12178841,  0.80305282,  0.01341783]]), 'b2': array([[-0.85454647]])}\n",
      "{'W0': array([[ 0.4397353 ,  0.13946076,  0.38609288,  0.91372982],\n",
      "       [-0.24257633, -0.21286012, -0.03862438, -0.66794582],\n",
      "       [ 0.48982572,  0.10052122,  0.14858537, -0.0208713 ],\n",
      "       [-0.92443522,  0.33961631,  0.31946295,  0.65818001],\n",
      "       [-0.51604719, -0.96467194, -0.68542684,  0.19943315]]), 'b0': array([[-0.81318229],\n",
      "       [ 0.98997738],\n",
      "       [-0.77834737],\n",
      "       [-0.84208993],\n",
      "       [-0.69657752]]), 'W1': array([[-0.39419433, -0.95396772,  0.31530952,  0.0306716 ,  0.86793069],\n",
      "       [ 0.53842793, -0.32532122,  0.7217266 , -0.43162925, -0.67367549],\n",
      "       [-0.88277064,  0.32219963, -0.51413552, -0.94593668, -0.94597963]]), 'b1': array([[-0.80457703],\n",
      "       [-0.13803331],\n",
      "       [ 0.48006214]]), 'W2': array([[0.7844714 , 0.28985478, 0.60081109]]), 'b2': array([[-0.14565043]])}\n",
      "{'W0': array([[-0.98634846, -0.03200355,  0.56632482, -0.02037898],\n",
      "       [ 0.97875507,  0.99439766, -0.33764616, -0.64946299],\n",
      "       [-0.14236023, -0.53755771, -0.32144024, -0.41726215],\n",
      "       [-0.42228389, -0.16742645,  0.03803032, -0.04260359],\n",
      "       [ 0.81826912,  0.92802434,  0.66493723,  0.71273838]]), 'b0': array([[ 0.54104898],\n",
      "       [ 0.95109786],\n",
      "       [-0.5340755 ],\n",
      "       [ 0.21390663],\n",
      "       [ 0.6930524 ]]), 'W1': array([[ 0.49849682,  0.52275454,  0.77954659, -0.97253945,  0.53186269],\n",
      "       [ 0.90627521,  0.03827916, -0.68527726,  0.33693246, -0.85568056],\n",
      "       [-0.87361503, -0.15370408, -0.53652599,  0.57562897,  0.25767332]]), 'b1': array([[ 0.28085159],\n",
      "       [-0.49134529],\n",
      "       [ 0.9940038 ]]), 'W2': array([[-0.45917617, -0.43982146, -0.19747266]]), 'b2': array([[-0.37539642]])}\n",
      "{'W0': array([[ 0.56501931, -0.28288954, -0.20433939, -0.23483139],\n",
      "       [ 0.65195933,  0.87861027,  0.61877343, -0.5865199 ],\n",
      "       [-0.91177088, -0.08311141,  0.62811309, -0.21314831],\n",
      "       [ 0.87775005, -0.57277554, -0.1793297 , -0.88298991],\n",
      "       [-0.52216972, -0.77337672, -0.18941317,  0.773722  ]]), 'b0': array([[-0.46703189],\n",
      "       [-0.36264318],\n",
      "       [ 0.7558421 ],\n",
      "       [ 0.48161618],\n",
      "       [-0.71144859]]), 'W1': array([[-0.69490993,  0.12732964,  0.46158056,  0.41648467,  0.62810518],\n",
      "       [-0.58815426, -0.9520857 ,  0.69881018, -0.55836959,  0.90944107],\n",
      "       [ 0.51014157, -0.48003059, -0.56345324, -0.57426728, -0.6314455 ]]), 'b1': array([[0.39014919],\n",
      "       [0.32002452],\n",
      "       [0.01272689]]), 'W2': array([[ 0.08631319, -0.83427561,  0.52259767]]), 'b2': array([[0.74624265]])}\n",
      "{'W0': array([[-0.47528513, -0.11124668, -0.0467095 ,  0.95209251],\n",
      "       [ 0.14838979, -0.65463917,  0.50427661,  0.7138921 ],\n",
      "       [ 0.68692496, -0.33903139, -0.59318653,  0.37688995],\n",
      "       [ 0.79173362, -0.95038977, -0.7937199 ,  0.74866603],\n",
      "       [ 0.94141198, -0.12142048,  0.77295052, -0.08550036]]), 'b0': array([[ 0.3763063 ],\n",
      "       [-0.45128825],\n",
      "       [ 0.48607344],\n",
      "       [ 0.9042284 ],\n",
      "       [ 0.28117824]]), 'W1': array([[-0.45228201, -0.37326517, -0.26015118, -0.02709605, -0.3373456 ],\n",
      "       [ 0.14880554, -0.17929481, -0.58934262, -0.14273555, -0.38555496],\n",
      "       [-0.54913246,  0.86401752, -0.76872281, -0.37403478, -0.12654857]]), 'b1': array([[ 0.10193244],\n",
      "       [-0.40247747],\n",
      "       [ 0.5108875 ]]), 'W2': array([[-0.51159236, -0.83383428, -0.78400509]]), 'b2': array([[-0.33304301]])}\n",
      "{'W0': array([[-0.24682024,  0.93868006, -0.04591448, -0.78508493],\n",
      "       [ 0.37298481,  0.83998856, -0.07731424,  0.63354331],\n",
      "       [ 0.66521596, -0.66036316,  0.13844573,  0.11849853],\n",
      "       [-0.54823669,  0.52756325,  0.31730485, -0.38467356],\n",
      "       [-0.15081646,  0.5220929 ,  0.57355748, -0.74358761]]), 'b0': array([[-0.96599626],\n",
      "       [-0.61871522],\n",
      "       [ 0.05492979],\n",
      "       [ 0.5150229 ],\n",
      "       [-0.02630165]]), 'W1': array([[-0.44797357,  0.1080068 ,  0.88015141, -0.92164691, -0.97833424],\n",
      "       [ 0.83900918, -0.04243231, -0.19217924, -0.35182579, -0.68881684],\n",
      "       [ 0.1458748 ,  0.29180165, -0.12059312, -0.20814764,  0.81768584]]), 'b1': array([[0.39953713],\n",
      "       [0.70567843],\n",
      "       [0.7591557 ]]), 'W2': array([[0.86470228, 0.23163174, 0.93431034]]), 'b2': array([[-0.43499274]])}\n",
      "{'W0': array([[-0.69042903, -0.99816102, -0.97221448,  0.62252293],\n",
      "       [ 0.55548242,  0.54973942, -0.7976599 , -0.72174861],\n",
      "       [-0.72192065, -0.12895897, -0.12952778, -0.37938352],\n",
      "       [-0.75053435, -0.15234114,  0.78642066,  0.11762909],\n",
      "       [ 0.76107952,  0.62017051,  0.76361589, -0.89970632]]), 'b0': array([[-0.80227808],\n",
      "       [ 0.76440323],\n",
      "       [-0.18157185],\n",
      "       [-0.47689243],\n",
      "       [ 0.79109638]]), 'W1': array([[-0.27291356, -0.92589504, -0.63918846,  0.92406029, -0.58063101],\n",
      "       [ 0.57963986,  0.39869224, -0.52912349,  0.4348915 ,  0.29151156],\n",
      "       [-0.70927273, -0.71757439,  0.2010076 ,  0.74852546,  0.47985357]]), 'b1': array([[ 0.57039064],\n",
      "       [-0.00309625],\n",
      "       [-0.79969526]]), 'W2': array([[ 0.41111933,  0.28937096, -0.87614159]]), 'b2': array([[-0.24409882]])}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W0': array([[-0.31742505,  0.19030954,  0.54906787, -0.10814022],\n",
      "       [ 0.13239617, -0.02204523,  0.89199431, -0.64041182],\n",
      "       [-0.5743245 , -0.417679  ,  0.23937154, -0.35947256],\n",
      "       [-0.75882796, -0.41922082, -0.51663949,  0.6230304 ],\n",
      "       [ 0.30339093,  0.97261269,  0.29305332,  0.14665797]]), 'b0': array([[-0.4604449 ],\n",
      "       [ 0.80074941],\n",
      "       [ 0.17104602],\n",
      "       [ 0.55163501],\n",
      "       [ 0.25895921]]), 'W1': array([[-0.69367001,  0.0014289 , -0.70783176,  0.84418811, -0.42572181],\n",
      "       [-0.20013239,  0.41998385, -0.17210428,  0.93355177, -0.1255241 ],\n",
      "       [-0.35455912,  0.51121908,  0.44456802, -0.0749294 , -0.84979788]]), 'b1': array([[0.50927666],\n",
      "       [0.34194342],\n",
      "       [0.23347517]]), 'W2': array([[ 0.15469558, -0.79187378, -0.37153225]]), 'b2': array([[-0.19673166]])}\n",
      "{'W0': array([[ 0.19319916,  0.6861412 ,  0.43184422, -0.70233428],\n",
      "       [ 0.97426809,  0.52278163,  0.08109323,  0.65411464],\n",
      "       [ 0.1531414 ,  0.91857194, -0.96602355, -0.53766608],\n",
      "       [ 0.11404213, -0.7352651 ,  0.62394801,  0.95523179],\n",
      "       [-0.10035552,  0.81130586, -0.42869368, -0.7500054 ]]), 'b0': array([[-0.37132389],\n",
      "       [-0.89619134],\n",
      "       [-0.59074459],\n",
      "       [-0.81323143],\n",
      "       [-0.83369197]]), 'W1': array([[ 0.19519877, -0.82949116,  0.47489689, -0.42331576, -0.09327812],\n",
      "       [ 0.88114425,  0.11873898, -0.03723474,  0.78051284, -0.91697746],\n",
      "       [ 0.82421406,  0.87266556,  0.51807224, -0.99052633, -0.26842802]]), 'b1': array([[-0.60170203],\n",
      "       [-0.80072385],\n",
      "       [ 0.22380893]]), 'W2': array([[0.66490314, 0.93692391, 0.4154043 ]]), 'b2': array([[-0.41492957]])}\n",
      "{'W0': array([[-0.85580501,  0.22432718,  0.035358  ,  0.37504826],\n",
      "       [ 0.59863003, -0.09913026, -0.1527568 ,  0.9827948 ],\n",
      "       [ 0.52883116, -0.05467539, -0.83152392,  0.99014346],\n",
      "       [-0.75687186,  0.94981248,  0.31049959,  0.51597802],\n",
      "       [ 0.78396933,  0.61636233, -0.65823641, -0.61237774]]), 'b0': array([[-0.04535489],\n",
      "       [-0.81156178],\n",
      "       [-0.2991622 ],\n",
      "       [-0.94325625],\n",
      "       [ 0.55918318]]), 'W1': array([[-0.84216004, -0.02429853,  0.30776336, -0.03498332, -0.31362289],\n",
      "       [-0.22816642,  0.88017529,  0.90088338, -0.60126071,  0.78631118],\n",
      "       [-0.41972671,  0.13013691, -0.44358619, -0.75490891,  0.14861784]]), 'b1': array([[-0.98681956],\n",
      "       [ 0.32567657],\n",
      "       [ 0.79491037]]), 'W2': array([[-0.99855629,  0.96388728,  0.93529053]]), 'b2': array([[0.66927878]])}\n",
      "{'W0': array([[ 0.14996335,  0.06578144, -0.39870352, -0.31883352],\n",
      "       [ 0.47464092, -0.97794649, -0.83773566, -0.27506665],\n",
      "       [-0.53042351, -0.16523229, -0.51073778,  0.24334163],\n",
      "       [ 0.65901254, -0.75025155, -0.71102726,  0.45275801],\n",
      "       [-0.78528877, -0.75346287, -0.04155959, -0.88967305]]), 'b0': array([[ 0.55530041],\n",
      "       [-0.08364922],\n",
      "       [ 0.99793457],\n",
      "       [-0.87292985],\n",
      "       [ 0.24046629]]), 'W1': array([[-0.84431312,  0.11724955,  0.80614827, -0.96914145, -0.69498012],\n",
      "       [-0.31383693,  0.49591097, -0.96223598, -0.95978261, -0.04756601],\n",
      "       [-0.44076381, -0.91515693, -0.64528403, -0.98571243, -0.11812733]]), 'b1': array([[-0.79513877],\n",
      "       [ 0.74023758],\n",
      "       [ 0.59089496]]), 'W2': array([[-0.77806822, -0.23814729, -0.87082613]]), 'b2': array([[0.89519702]])}\n",
      "{'W0': array([[ 0.95310245,  0.39212146, -0.40648429,  0.72166186],\n",
      "       [-0.11923826,  0.30298803, -0.94167683,  0.36171006],\n",
      "       [ 0.70126308,  0.49663442,  0.51523913, -0.77679993],\n",
      "       [ 0.33380469,  0.39959178, -0.58182839, -0.35130238],\n",
      "       [-0.97337019,  0.04785341, -0.99094803, -0.23524914]]), 'b0': array([[-0.34035117],\n",
      "       [ 0.7795352 ],\n",
      "       [ 0.017374  ],\n",
      "       [ 0.62198429],\n",
      "       [ 0.5047523 ]]), 'W1': array([[-0.6331562 , -0.13246611, -0.60810833,  0.85603228, -0.17048492],\n",
      "       [ 0.04736436,  0.45958769,  0.26431584, -0.14429006,  0.41684158],\n",
      "       [-0.55276139, -0.93615731,  0.25573633, -0.96622255,  0.89614917]]), 'b1': array([[ 0.10161851],\n",
      "       [-0.05532847],\n",
      "       [-0.03557613]]), 'W2': array([[0.44674773, 0.30734361, 0.40511218]]), 'b2': array([[0.64884888]])}\n",
      "{'W0': array([[-0.25483426, -0.77215719,  0.25904778,  0.59260368],\n",
      "       [-0.29772507,  0.31656882, -0.41635659,  0.4971193 ],\n",
      "       [ 0.25200974,  0.43786525, -0.96904709,  0.28503931],\n",
      "       [ 0.871131  , -0.27462152, -0.16887347,  0.12845428],\n",
      "       [-0.36700856,  0.77542003,  0.35159529,  0.96855654]]), 'b0': array([[-0.88734501],\n",
      "       [-0.65466157],\n",
      "       [-0.82386032],\n",
      "       [-0.73800285],\n",
      "       [ 0.75875501]]), 'W1': array([[ 0.96787019,  0.13678838,  0.9054478 ,  0.46038556,  0.75973038],\n",
      "       [ 0.09400482,  0.17604313, -0.52818233,  0.73349685,  0.90896119],\n",
      "       [ 0.36484333,  0.2113755 ,  0.08166429,  0.5615359 ,  0.35212392]]), 'b1': array([[-0.79990074],\n",
      "       [ 0.84950202],\n",
      "       [ 0.79015552]]), 'W2': array([[ 0.31162952, -0.02675465, -0.83028171]]), 'b2': array([[0.59580349]])}\n",
      "({'W0': array([[ 0.05549177,  0.86660708, -0.58753344,  0.77657216],\n",
      "       [-0.2646353 , -0.26855846, -0.35382098, -0.85681464],\n",
      "       [ 0.79073786,  0.49653627, -0.21589045, -0.64882158],\n",
      "       [ 0.18758145, -0.38061365,  0.20189859,  0.39520032],\n",
      "       [ 0.09886371,  0.9867457 ,  0.69085411,  0.91734613]]), 'b0': array([[-0.00201551],\n",
      "       [-0.44262795],\n",
      "       [-0.93591664],\n",
      "       [-0.27916255],\n",
      "       [ 0.3492006 ]]), 'W1': array([[ 5.48131541e-01, -2.14630676e-02,  3.03032326e-01,\n",
      "         1.08737617e-01, -9.74867947e-01],\n",
      "       [ 1.01261398e-01,  6.23923422e-01,  9.84723133e-01,\n",
      "        -9.23861192e-01, -1.39514589e-04],\n",
      "       [-3.54184733e-01,  7.48677333e-01,  2.04852662e-01,\n",
      "        -4.72377019e-01,  6.22522490e-01]]), 'b1': array([[-0.24500544],\n",
      "       [-0.2074397 ],\n",
      "       [-0.75449038]]), 'W2': array([[-0.12178841,  0.80305282,  0.01341783]]), 'b2': array([[-0.85454647]])}, 0.555084972926607) best res\n"
     ]
    }
   ],
   "source": [
    "from BO_methods import CustomTask  # Import CustomTask from BO_methods module\n",
    "from config_NN_methods import flex_NN, ActivationFunctions\n",
    "from BO_pipeline import WeightAndBiasOptimizer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from mcbo.optimizers.bo_builder import BoBuilder\n",
    "import numpy as np\n",
    "\n",
    "# Define the Rastrigin function in 4 dimensions\n",
    "def rastrigin_4d(x, y, z, w, A=10):\n",
    "    return (4 * A + (x**2 - A * np.cos(2 * np.pi * x)) + \n",
    "            (y**2 - A * np.cos(2 * np.pi * y)) +\n",
    "            (z**2 - A * np.cos(2 * np.pi * z)) +\n",
    "            (w**2 - A * np.cos(2 * np.pi * w)))\n",
    "\n",
    "# Desired dataset length\n",
    "desired_length = 300\n",
    "\n",
    "# Calculate the number of points for each dimension\n",
    "num_points_per_dimension = int(np.sqrt(desired_length))\n",
    "\n",
    "# Generate X coordinates with the calculated number of points\n",
    "x = np.linspace(-5.12, 5.12, num_points_per_dimension)\n",
    "y = np.linspace(-5.12, 5.12, num_points_per_dimension)\n",
    "z = np.linspace(-5.12, 5.12, num_points_per_dimension)\n",
    "w = np.linspace(-5.12, 5.12, num_points_per_dimension)\n",
    "X, Y, Z, W = np.meshgrid(x, y, z, w, indexing='ij')\n",
    "Z_rastrigin = rastrigin_4d(X, Y, Z, W)\n",
    "\n",
    "# Flatten X, Y, Z, and W\n",
    "X = np.column_stack((X.flatten(), Y.flatten(), Z.flatten(), W.flatten()))\n",
    "\n",
    "# Define a binary threshold for target variable\n",
    "threshold = np.mean(Z_rastrigin)  # You can adjust the threshold as needed\n",
    "\n",
    "# Generate binary target variable based on the threshold\n",
    "y = (Z_rastrigin.flatten() > threshold).astype(int)\n",
    "\n",
    "# Display the shapes of X and y\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y_binary:\", y.shape)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "# Usage:\n",
    "layer_sizes = [4,5,3,1]   # Define your layer sizes\n",
    "\n",
    "flex_NN_obj = flex_NN(layer_sizes, X_train, y_train)\n",
    "activation_obj = ActivationFunctions()\n",
    "\n",
    "parameters = flex_NN_obj.label_parameters()\n",
    "\n",
    "hidden_activation = activation_obj.relu\n",
    "output_activation = activation_obj.sigmoid\n",
    "\n",
    "\n",
    "task = CustomTask(layer_sizes, X_train, y_train)  # Instantiate CustomTask from BO_methods module\n",
    "searchspace = task.get_search_space()\n",
    "\n",
    "optimizer_builder = BoBuilder(model_id='gp_rd', acq_opt_id='is', acq_func_id='ei', tr_id='basic')\n",
    "opt = optimizer_builder.build_bo(search_space=searchspace, n_init=300)\n",
    "print('opt ========', opt)\n",
    "\n",
    "weight_bias_optimizer = WeightAndBiasOptimizer(task=task, optimizer=opt)\n",
    "\n",
    "best_results = weight_bias_optimizer.find_best_weights_and_biases(\n",
    "                                layer_sizes, 30, X_train, y_train)\n",
    "print(best_results, 'best res')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe1c7ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867de485",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d2679fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83521"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "25fc49e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.320508075688775"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b382ace8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
